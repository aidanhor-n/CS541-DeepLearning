{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8be85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3634a2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96d22dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4439739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770b774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d3a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f1ea2f-0796-437f-bac8-0be319b3caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, random_seed = 42):\n",
    "    train_split = 0.8\n",
    "    random_seed = 42\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    validation_split = .2\n",
    "    \n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(train_indices)\n",
    "    random.shuffle(val_indices)\n",
    "    \n",
    "    train = [dataset[i] for i in train_indices]\n",
    "    val = [dataset[j] for j in val_indices]\n",
    "    \n",
    "    return train, val\n",
    "\n",
    "#any(word in train for word in val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd05b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2803\n",
      "fra 4345\n"
     ]
    }
   ],
   "source": [
    "###ENGLISH TO ENGLISH WORKAROUND\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareAEData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[0])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    pairs = [[v,v] for v,k in pairs]\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3e89af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they re dead .', 'elles sont decedees .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fe917-6002-4443-bef2-71f481715fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8870ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ad5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de08ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "062bda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7527ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cbf81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe73920",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f13ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, enc_learning_rate = 0.01, dec_learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "        \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=enc_learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=dec_learning_rate)\n",
    "    \n",
    "    train_, val_ = train_test_split(pairs, random_seed = 42)\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(train_))\n",
    "                      for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25d6ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77364c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "633cdf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97f3e9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 27s (- 8m 39s) (1000 5%) 4.2078\n",
      "0m 59s (- 8m 54s) (2000 10%) 3.6780\n",
      "1m 27s (- 8m 13s) (3000 15%) 3.4738\n",
      "1m 58s (- 7m 54s) (4000 20%) 3.2387\n",
      "2m 27s (- 7m 22s) (5000 25%) 3.1660\n",
      "2m 58s (- 6m 56s) (6000 30%) 3.0595\n",
      "3m 27s (- 6m 24s) (7000 35%) 2.9013\n",
      "3m 59s (- 5m 59s) (8000 40%) 2.8575\n",
      "4m 29s (- 5m 29s) (9000 45%) 2.7806\n",
      "5m 1s (- 5m 1s) (10000 50%) 2.7396\n",
      "5m 30s (- 4m 30s) (11000 55%) 2.5351\n",
      "6m 0s (- 4m 0s) (12000 60%) 2.4768\n",
      "6m 29s (- 3m 29s) (13000 65%) 2.4918\n",
      "6m 59s (- 2m 59s) (14000 70%) 2.3806\n",
      "7m 28s (- 2m 29s) (15000 75%) 2.3577\n",
      "8m 2s (- 2m 0s) (16000 80%) 2.3057\n",
      "8m 31s (- 1m 30s) (17000 85%) 2.1871\n",
      "9m 1s (- 1m 0s) (18000 90%) 2.1282\n",
      "9m 30s (- 0m 30s) (19000 95%) 2.0965\n",
      "10m 1s (- 0m 0s) (20000 100%) 2.1175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size =EncoderRNN\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(encoder1, attn_decoder1, 20000, print_every=1000, plot_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99c86f8a-6c5b-472f-9cac-43e649f2e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2803\n",
      "fra 2803\n",
      "0m 37s (- 11m 58s) (1000 5%) 3.4621\n",
      "1m 16s (- 11m 31s) (2000 10%) 2.6391\n",
      "1m 53s (- 10m 43s) (3000 15%) 2.3344\n",
      "2m 29s (- 9m 56s) (4000 20%) 2.0306\n",
      "3m 7s (- 9m 23s) (5000 25%) 1.8655\n",
      "3m 42s (- 8m 38s) (6000 30%) 1.6410\n",
      "4m 18s (- 8m 0s) (7000 35%) 1.5584\n",
      "4m 56s (- 7m 24s) (8000 40%) 1.4047\n",
      "5m 36s (- 6m 50s) (9000 45%) 1.3019\n",
      "6m 14s (- 6m 14s) (10000 50%) 1.2099\n",
      "6m 52s (- 5m 37s) (11000 55%) 1.0736\n",
      "7m 28s (- 4m 58s) (12000 60%) 0.9735\n",
      "8m 5s (- 4m 21s) (13000 65%) 0.9636\n",
      "8m 45s (- 3m 45s) (14000 70%) 0.9072\n",
      "9m 25s (- 3m 8s) (15000 75%) 0.8515\n",
      "10m 5s (- 2m 31s) (16000 80%) 0.8301\n",
      "10m 44s (- 1m 53s) (17000 85%) 0.7379\n",
      "11m 24s (- 1m 16s) (18000 90%) 0.6642\n",
      "12m 2s (- 0m 38s) (19000 95%) 0.6279\n",
      "12m 38s (- 0m 0s) (20000 100%) 0.6552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "input_lang, output_lang, pairs = prepareAEData('eng', 'fra', False)\n",
    "\n",
    "AE_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "AE_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(AE_encoder, AE_decoder, 20000, print_every=1000, plot_every=1000)\n",
    "\n",
    "torch.save(AE_encoder.state_dict(), 'AE_encoder.dict')\n",
    "torch.save(AE_decoder.state_dict(), 'AE_decoder.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9254e525-a842-49e8-a15b-b7714f1a132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2803\n",
      "fra 4345\n",
      "0m 54s (- 17m 8s) (1000 5%) 4.0304\n",
      "1m 47s (- 16m 9s) (2000 10%) 3.5425\n",
      "2m 44s (- 15m 33s) (3000 15%) 3.3571\n",
      "3m 33s (- 14m 14s) (4000 20%) 3.1280\n",
      "4m 27s (- 13m 21s) (5000 25%) 3.0609\n",
      "5m 17s (- 12m 21s) (6000 30%) 2.9662\n",
      "6m 11s (- 11m 29s) (7000 35%) 2.7921\n",
      "7m 7s (- 10m 41s) (8000 40%) 2.7819\n",
      "8m 2s (- 9m 49s) (9000 45%) 2.6988\n",
      "8m 52s (- 8m 52s) (10000 50%) 2.6582\n",
      "9m 43s (- 7m 57s) (11000 55%) 2.5088\n",
      "10m 37s (- 7m 5s) (12000 60%) 2.4530\n",
      "11m 27s (- 6m 10s) (13000 65%) 2.4794\n",
      "12m 18s (- 5m 16s) (14000 70%) 2.4184\n",
      "13m 9s (- 4m 23s) (15000 75%) 2.3603\n",
      "14m 6s (- 3m 31s) (16000 80%) 2.3658\n",
      "15m 5s (- 2m 39s) (17000 85%) 2.2530\n",
      "15m 57s (- 1m 46s) (18000 90%) 2.2155\n",
      "16m 46s (- 0m 52s) (19000 95%) 2.2003\n",
      "17m 39s (- 0m 0s) (20000 100%) 2.2154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use AE_encoder and new decoder for translation\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(AE_encoder, attn_decoder1, 20000, print_every=1000, plot_every = 10, enc_learning_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe2745eb-4bf3-4b70-85fe-405ecd69dd29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on EncoderRNN in module __main__ object:\n",
      "\n",
      "class EncoderRNN(torch.nn.modules.module.Module)\n",
      " |  EncoderRNN(input_size, hidden_size)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EncoderRNN\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_size, hidden_size)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input, hidden)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  initHidden(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AE_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972f6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m the youngest child .\n",
      "= je suis la plus jeune enfant .\n",
      "< je suis le meme . <EOS>\n",
      "\n",
      "> she s my first love .\n",
      "= elle est mon premier amour .\n",
      "< elle est est le . . <EOS>\n",
      "\n",
      "> i m writing a book .\n",
      "= je suis en train d ecrire un bouquin .\n",
      "< je suis un train . . <EOS>\n",
      "\n",
      "> i m no longer angry at you .\n",
      "= je ne suis plus en colere apres vous .\n",
      "< je ne suis plus en colere . <EOS>\n",
      "\n",
      "> i m contagious .\n",
      "= je suis contagieuse .\n",
      "< je suis rejouis . <EOS>\n",
      "\n",
      "> i am convinced that he is innocent .\n",
      "= je suis convaincu de son innocence .\n",
      "< je suis certain qu il est <EOS>\n",
      "\n",
      "> i m in way over my head .\n",
      "= j ai des ennuis jusqu au cou .\n",
      "< je suis en . de ma voiture . . <EOS>\n",
      "\n",
      "> i m stubborn .\n",
      "= je suis obstine .\n",
      "< je suis en . <EOS>\n",
      "\n",
      "> they are collecting contributions for the church .\n",
      "= ils collectent des dons pour l eglise .\n",
      "< ils sont en train de la voyage . <EOS>\n",
      "\n",
      "> i m really sleepy .\n",
      "= j ai vraiment sommeil .\n",
      "< je suis vraiment vraiment . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(AE_encoder, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a16f876e-73b0-4810-bcb8-8c15c5eb6309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[-0.1075, -1.4561, -0.9269,  ..., -0.5095,  2.1325, -0.2530],\n",
       "                      [ 1.0590, -2.3225,  2.6499,  ..., -1.7026, -0.4011, -1.4441],\n",
       "                      [-0.3590,  0.9642,  0.0719,  ..., -0.8602, -2.1549,  0.8839],\n",
       "                      ...,\n",
       "                      [-1.6855,  1.0948,  0.1890,  ...,  1.2942,  0.6984, -0.0205],\n",
       "                      [-1.1715, -0.4992,  1.0895,  ..., -0.9463,  0.3902, -0.2431],\n",
       "                      [ 1.1411, -0.2259, -1.5470,  ...,  1.7607, -0.5163, -0.9703]])),\n",
       "             ('gru.weight_ih_l0',\n",
       "              tensor([[-0.0435,  0.0142, -0.0385,  ..., -0.0007,  0.0494,  0.0118],\n",
       "                      [ 0.0231,  0.0476,  0.0293,  ...,  0.0334,  0.0247,  0.0224],\n",
       "                      [-0.0404, -0.0577,  0.0246,  ..., -0.0609,  0.0534,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.1048,  0.0294,  ...,  0.0401, -0.0350,  0.1113],\n",
       "                      [ 0.0081,  0.0061, -0.0638,  ..., -0.0464,  0.0555, -0.0692],\n",
       "                      [ 0.0490, -0.0380,  0.0497,  ...,  0.0124,  0.0720,  0.1041]])),\n",
       "             ('gru.weight_hh_l0',\n",
       "              tensor([[ 0.0171,  0.0121,  0.0017,  ..., -0.0471, -0.0235, -0.0246],\n",
       "                      [-0.0179,  0.0523,  0.0339,  ...,  0.0275,  0.0382, -0.0182],\n",
       "                      [-0.0224, -0.0390,  0.0652,  ..., -0.0200, -0.0697,  0.0094],\n",
       "                      ...,\n",
       "                      [-0.0223, -0.0837,  0.0795,  ...,  0.0311, -0.0277,  0.0496],\n",
       "                      [-0.0581,  0.0142,  0.0759,  ...,  0.0372, -0.0651,  0.0396],\n",
       "                      [ 0.0078, -0.0013,  0.0367,  ..., -0.0395, -0.0420,  0.0046]])),\n",
       "             ('gru.bias_ih_l0',\n",
       "              tensor([ 3.0973e-02, -4.8310e-02,  6.9925e-02,  8.3475e-02,  6.8531e-02,\n",
       "                       2.2193e-02,  1.4898e-01,  5.0024e-02, -3.0826e-02,  2.0506e-02,\n",
       "                       1.0355e-02,  1.1282e-02,  8.3116e-02,  4.0010e-02, -3.4442e-02,\n",
       "                       3.1285e-02,  3.1425e-02, -5.5288e-03, -5.5873e-03,  1.2434e-01,\n",
       "                       2.8309e-02,  2.1427e-04,  8.2363e-02, -3.7961e-02,  4.6129e-02,\n",
       "                       6.7838e-02, -2.0762e-02,  9.8445e-02, -9.9223e-03,  5.2815e-02,\n",
       "                       2.6661e-02, -1.4395e-03,  5.2649e-02,  4.2422e-02,  2.9512e-02,\n",
       "                      -2.7055e-02, -2.6372e-02,  9.3036e-03, -2.1571e-03,  1.8847e-03,\n",
       "                      -2.5345e-02, -6.7821e-03,  5.0284e-02, -1.2652e-02,  2.9727e-02,\n",
       "                       3.1625e-02,  5.2020e-02, -4.1657e-02,  3.2806e-02,  2.3711e-02,\n",
       "                       4.4365e-02,  5.0680e-02,  5.0452e-02, -1.3460e-02, -3.8177e-02,\n",
       "                       5.2154e-02,  3.7765e-02, -5.0237e-02,  8.1771e-02, -4.1957e-02,\n",
       "                      -4.7043e-02,  5.2669e-02, -1.4969e-02,  9.1032e-02,  6.0428e-02,\n",
       "                       3.0585e-02,  1.2259e-01,  6.8840e-02, -3.3964e-02, -2.1146e-02,\n",
       "                       5.4892e-02,  3.5278e-02, -2.6426e-02,  1.4018e-02,  4.7429e-02,\n",
       "                       2.3443e-02,  6.2397e-02, -3.7459e-02,  3.6675e-03,  2.6768e-02,\n",
       "                      -3.6832e-02,  2.2561e-02,  2.9001e-02,  5.8717e-02,  5.4169e-02,\n",
       "                      -2.3884e-02,  5.2052e-02, -2.6198e-03, -7.6988e-03, -3.9603e-02,\n",
       "                       1.5400e-02,  3.4957e-02,  9.7899e-02, -3.4316e-02, -1.0079e-02,\n",
       "                       6.7788e-02,  6.2233e-02,  5.9221e-02, -5.0395e-02, -4.2250e-02,\n",
       "                       4.6767e-02,  2.4103e-02, -8.7490e-03,  2.1632e-02,  7.9259e-02,\n",
       "                       3.8676e-02,  2.8144e-02,  4.3903e-02, -1.3396e-02, -1.0908e-02,\n",
       "                       4.0307e-02,  5.4926e-02,  4.6653e-02, -3.0030e-03,  1.6358e-03,\n",
       "                       5.9264e-02,  4.6087e-02,  4.7669e-02, -3.1855e-02,  3.2350e-03,\n",
       "                      -1.5149e-02,  1.7265e-02,  8.0106e-02,  8.6505e-02,  6.9860e-02,\n",
       "                      -3.6001e-03, -3.1422e-02,  1.8473e-02,  6.7385e-02,  3.7461e-02,\n",
       "                       5.5395e-02,  5.7843e-02,  7.6955e-02,  2.2788e-02,  4.4649e-02,\n",
       "                      -2.9816e-02, -2.5901e-02,  1.1752e-01, -3.9728e-04,  3.5645e-02,\n",
       "                       7.9939e-02,  2.4673e-02, -5.9999e-02,  4.2680e-02, -9.9992e-03,\n",
       "                       4.0236e-02,  6.6899e-02,  7.2811e-03, -4.0078e-03,  3.0496e-02,\n",
       "                      -9.1282e-03,  7.1630e-02,  6.3693e-02,  3.7094e-02,  1.1349e-01,\n",
       "                      -1.7691e-02, -3.3975e-02,  5.1992e-02, -2.4725e-02,  3.6947e-02,\n",
       "                      -8.2738e-03,  2.0678e-02, -3.4330e-02, -4.2139e-02,  2.2958e-02,\n",
       "                       1.9207e-02, -1.0386e-02, -3.7414e-02,  8.5089e-02, -4.5815e-03,\n",
       "                       4.2611e-02, -1.6832e-03,  1.0702e-02,  1.6211e-02, -2.3345e-04,\n",
       "                       3.3662e-02,  6.2863e-02, -3.5538e-02,  3.6953e-02,  5.6600e-02,\n",
       "                       3.1933e-02,  5.5476e-02, -2.9507e-02, -5.6525e-02,  1.2034e-02,\n",
       "                      -1.1156e-02, -2.1684e-02,  5.3204e-02,  5.4113e-02,  5.8928e-02,\n",
       "                       7.6794e-02, -4.1446e-02, -4.2342e-02,  5.3719e-02, -1.6490e-02,\n",
       "                      -2.2003e-02,  2.3000e-02,  6.2461e-02,  6.1886e-02, -3.6537e-02,\n",
       "                      -4.0628e-02,  1.0260e-01,  7.4059e-02,  1.8195e-02, -1.0334e-02,\n",
       "                       8.8897e-02, -9.0179e-03,  5.6320e-03,  9.0578e-03,  4.6681e-02,\n",
       "                       3.6690e-02, -1.8956e-02,  1.6376e-02,  5.1023e-02, -2.2552e-02,\n",
       "                      -3.2809e-02, -2.4664e-02,  3.1770e-02, -3.4288e-02,  1.0660e-02,\n",
       "                       6.3905e-02,  5.0060e-02,  1.8881e-02,  2.9570e-02,  5.7554e-02,\n",
       "                      -8.3631e-03, -6.1744e-03,  9.7494e-02,  1.2623e-02,  5.5275e-04,\n",
       "                       2.9502e-02,  3.6682e-02, -1.1881e-02,  8.9968e-02,  5.1912e-02,\n",
       "                       2.2340e-02,  5.8894e-02,  9.7203e-03,  6.2132e-02,  5.4779e-02,\n",
       "                       3.9853e-02, -2.3788e-02,  3.2569e-02,  8.8725e-02,  5.2161e-03,\n",
       "                      -2.5444e-02, -5.4180e-02,  7.1539e-02,  2.4261e-02, -2.7525e-02,\n",
       "                       1.3255e-02,  9.7390e-02,  3.1559e-02,  1.6374e-02,  6.6830e-02,\n",
       "                       4.7039e-02, -6.1270e-02, -9.3830e-02, -3.7698e-02, -2.3536e-01,\n",
       "                      -3.8554e-02, -1.1713e-01,  1.7577e-01, -1.1839e-01,  4.4562e-02,\n",
       "                      -7.7686e-03, -9.1764e-02, -1.0155e-01, -1.2389e-01,  5.9429e-03,\n",
       "                      -1.2181e-01, -1.3172e-01, -2.4267e-02, -1.1209e-02, -5.6128e-02,\n",
       "                      -1.3858e-01, -2.5702e-02, -1.6680e-03, -1.0435e-01, -1.0553e-01,\n",
       "                      -6.2173e-03, -3.3512e-02, -1.8718e-02, -3.8179e-03,  3.7230e-02,\n",
       "                      -8.1539e-02, -1.5832e-01, -2.5796e-02, -4.4829e-02, -9.3723e-02,\n",
       "                       3.5385e-02, -8.1218e-02,  4.0679e-03,  2.2178e-03, -1.0192e-01,\n",
       "                      -8.6588e-02, -9.0904e-02, -1.0401e-01, -1.1399e-01,  2.3196e-02,\n",
       "                      -1.9894e-02, -1.5619e-02, -5.4635e-02, -6.2189e-02, -2.9502e-02,\n",
       "                      -3.5750e-03, -1.4086e-01,  6.4772e-03, -5.5059e-03, -7.6179e-02,\n",
       "                      -8.1077e-02, -4.5923e-02, -1.9108e-02, -8.8737e-02, -1.7039e-02,\n",
       "                      -8.1062e-02, -8.3540e-02, -3.0798e-02, -1.2527e-01, -5.6161e-03,\n",
       "                      -1.2550e-01, -1.0998e-02, -1.8030e-01, -3.8618e-02, -4.3450e-02,\n",
       "                      -9.1300e-02, -5.1501e-02,  1.8569e-02, -6.9147e-02, -2.1448e-03,\n",
       "                      -9.1001e-02, -4.9085e-02, -6.7796e-02,  1.8831e-02, -1.0528e-01,\n",
       "                      -8.0865e-02, -1.1945e-02,  5.8342e-02, -1.2550e-01, -8.1618e-02,\n",
       "                      -6.5294e-02, -2.8351e-03, -1.0421e-01,  1.6912e-02, -4.9571e-02,\n",
       "                      -4.1358e-02, -7.6102e-02, -7.5397e-02, -1.2346e-01,  2.0210e-03,\n",
       "                      -2.7455e-02, -8.7476e-02, -7.5364e-02, -4.4412e-02, -3.1407e-02,\n",
       "                       3.1191e-02, -2.4310e-02, -4.2655e-02, -9.2870e-02,  4.3308e-03,\n",
       "                      -6.6845e-02, -5.8023e-02, -7.5334e-02, -2.5190e-02, -3.8103e-02,\n",
       "                      -5.8836e-02, -1.3448e-01, -7.5101e-03, -2.1507e-02, -8.9779e-02,\n",
       "                      -4.7189e-02, -1.3710e-01, -9.2455e-02,  2.5287e-03, -8.5002e-02,\n",
       "                      -1.2464e-01,  1.0077e-02, -8.2457e-03, -2.0569e-01, -5.3995e-02,\n",
       "                       1.2481e-02, -6.7420e-02, -7.6732e-02, -9.4608e-02, -2.4566e-02,\n",
       "                      -9.3275e-02, -7.4274e-02, -5.6131e-02, -5.6158e-02, -7.1771e-02,\n",
       "                      -7.3791e-03, -7.5715e-02, -2.7093e-02, -5.2455e-02,  6.2025e-03,\n",
       "                      -1.1397e-01, -7.9458e-02, -1.1618e-01, -7.9421e-02,  2.0462e-02,\n",
       "                      -8.3434e-02,  1.6223e-02, -4.5310e-02, -4.2014e-02,  1.8927e-03,\n",
       "                      -6.7138e-02, -1.2286e-01, -4.8179e-02, -2.1413e-02, -9.9877e-02,\n",
       "                      -7.8848e-02,  1.1653e-02, -4.6316e-02, -8.4587e-02, -6.8256e-02,\n",
       "                      -1.3256e-01, -5.5269e-02, -9.2310e-02, -9.8704e-02, -9.4953e-02,\n",
       "                      -7.4802e-02, -8.4000e-02, -9.9535e-02,  3.9711e-03, -6.6620e-02,\n",
       "                      -5.7456e-02, -3.6228e-02, -4.6339e-02,  5.9419e-03,  2.5709e-02,\n",
       "                       4.6245e-04,  2.3790e-03, -1.2164e-01,  5.6749e-02, -2.5829e-03,\n",
       "                      -1.0634e-02, -4.4489e-02, -2.4458e-02, -1.2312e-01,  1.4779e-02,\n",
       "                      -1.0042e-01,  1.3233e-02, -4.5185e-02, -3.4653e-03,  5.5758e-03,\n",
       "                      -5.1099e-02,  4.2460e-02, -2.9868e-02, -1.1615e-01, -6.2874e-02,\n",
       "                      -5.4523e-02, -7.8886e-02, -1.5897e-01,  3.1862e-03, -5.9269e-02,\n",
       "                      -2.0114e-02, -9.5583e-02, -7.9009e-02, -1.5459e-02, -8.1869e-02,\n",
       "                      -1.3593e-02, -1.0124e-01, -1.0910e-01,  2.7680e-02, -5.9500e-02,\n",
       "                       1.1095e-02,  2.5280e-03, -1.0579e-01, -8.9330e-03, -3.7396e-02,\n",
       "                      -5.4802e-02,  2.6132e-02, -9.7218e-03, -8.1800e-02,  5.5493e-02,\n",
       "                      -7.8332e-02,  3.3411e-02, -1.1560e-01, -8.2293e-02, -3.3887e-02,\n",
       "                      -5.4436e-02, -3.9134e-02, -1.0384e-01,  1.9306e-02, -4.0848e-02,\n",
       "                      -7.1774e-02, -5.3316e-02, -2.6519e-02, -8.8617e-02, -8.2551e-02,\n",
       "                      -1.2854e-01, -3.1554e-02, -6.2312e-02, -4.8592e-02, -5.2150e-02,\n",
       "                      -8.4253e-02,  4.4243e-02, -1.5415e-02, -4.1127e-02, -6.1024e-02,\n",
       "                      -1.0835e-01, -1.8361e-02, -5.4564e-02, -1.3099e-01, -1.2356e-01,\n",
       "                      -5.7569e-02, -3.4837e-02, -9.5550e-02, -8.0488e-02, -1.9145e-02,\n",
       "                      -1.1218e-01, -5.0645e-02,  7.4919e-02, -2.4268e-02,  2.8064e-01,\n",
       "                      -2.3048e-02, -1.4407e-01, -1.0909e-01, -3.1238e-01, -1.0985e-01,\n",
       "                      -1.8121e-02, -2.5806e-02,  2.3559e-02, -7.3675e-02, -1.1081e-01,\n",
       "                       3.7304e-02, -5.9285e-02, -6.9644e-03, -3.3516e-02, -6.1883e-02,\n",
       "                      -1.4977e-01,  1.5446e-01, -3.2190e-02,  1.7055e-01,  6.1997e-03,\n",
       "                      -9.1644e-02,  5.3933e-02, -2.5574e-02,  1.9956e-01, -1.3338e-01,\n",
       "                       1.1853e-01,  9.8209e-02,  1.1299e-01,  2.3894e-01,  1.3758e-01,\n",
       "                       2.5153e-02, -2.2786e-01,  2.4864e-02, -1.1268e-01, -1.5830e-01,\n",
       "                       4.8416e-02,  1.0090e-01, -2.5867e-04, -2.0452e-02, -1.1597e-01,\n",
       "                      -1.1335e-01, -1.1092e-01, -6.8242e-02,  1.6519e-01,  8.1336e-02,\n",
       "                       1.3009e-01,  1.2555e-02,  2.1345e-01, -1.0213e-01,  3.1776e-02,\n",
       "                      -4.9541e-02, -8.9672e-02, -3.7207e-02,  7.0153e-02, -4.6940e-02,\n",
       "                      -8.8087e-02,  3.7649e-02,  5.1981e-02, -5.6114e-02, -1.9099e-02,\n",
       "                       1.8535e-02,  4.8469e-02, -5.3935e-02,  1.9487e-02,  6.8070e-02,\n",
       "                       3.8362e-02,  5.3326e-02, -9.0704e-02,  1.1723e-02, -1.0153e-02,\n",
       "                      -1.2888e-02,  2.0924e-01, -6.4152e-02, -1.3094e-01, -6.9768e-02,\n",
       "                       5.9834e-02,  2.6752e-03,  1.3100e-02,  1.5930e-01, -1.6344e-01,\n",
       "                      -2.9345e-02,  3.7192e-02, -5.2641e-02,  5.3412e-02, -2.4532e-02,\n",
       "                      -1.0695e-01,  8.3791e-02, -1.7759e-01,  9.2454e-02,  1.6645e-02,\n",
       "                      -4.4382e-02,  6.9884e-02,  5.0523e-02, -4.6382e-02,  8.6608e-02,\n",
       "                       5.6979e-02,  4.0885e-02, -1.0039e-01,  1.3022e-02,  1.6698e-02,\n",
       "                       4.6792e-02, -6.4869e-02,  8.8206e-02,  1.1169e-02,  5.2261e-02,\n",
       "                      -2.1247e-01,  5.2677e-02,  9.4796e-02, -5.8519e-02, -5.3133e-02,\n",
       "                       9.0620e-02, -6.0390e-02,  8.0790e-03, -1.2445e-01,  1.1867e-02,\n",
       "                      -1.1712e-01,  6.0555e-03, -6.5715e-02,  3.7100e-02,  1.0535e-01,\n",
       "                       1.0600e-01, -9.7980e-02,  6.6321e-03, -9.1769e-02, -5.5014e-02,\n",
       "                      -8.7816e-02, -1.0135e-02,  6.3305e-03,  5.4908e-02, -5.6107e-02,\n",
       "                      -1.6338e-01,  6.4779e-02, -1.1851e-01,  7.9862e-02, -1.6619e-01,\n",
       "                       4.2845e-02, -1.4205e-02,  3.8753e-02,  2.7318e-01, -1.0470e-01,\n",
       "                      -1.9855e-02, -7.5490e-02,  7.3020e-02,  7.6088e-02,  5.4358e-02,\n",
       "                      -1.2050e-01,  1.0944e-01,  6.1827e-02, -1.1933e-01,  5.0185e-02,\n",
       "                      -7.3072e-02, -1.2359e-01, -4.4314e-02,  4.5311e-02,  5.3045e-02,\n",
       "                      -5.5919e-02,  2.2072e-02, -4.4253e-02,  3.9614e-02,  1.2768e-02,\n",
       "                      -6.0684e-02, -9.2866e-02, -1.3613e-02,  6.0721e-02,  3.3449e-02,\n",
       "                      -1.3632e-01,  1.1857e-01,  3.6806e-02, -1.2030e-01,  1.0041e-01,\n",
       "                      -2.0822e-01, -2.8895e-02, -5.1834e-02, -2.5027e-02, -6.6510e-02,\n",
       "                      -8.5927e-02,  1.9339e-01, -2.8578e-02,  8.3812e-02,  4.5714e-02,\n",
       "                      -1.7666e-02, -8.9172e-03,  6.7074e-02, -1.2611e-01,  3.3761e-02,\n",
       "                       9.7322e-02,  5.6637e-02, -4.1444e-02,  6.8806e-02,  6.6332e-02,\n",
       "                      -1.1724e-01, -1.8774e-02, -3.6366e-02,  1.1739e-01, -9.7627e-02,\n",
       "                       7.6434e-02,  8.9573e-02, -4.2590e-02, -7.2800e-02, -1.1720e-01,\n",
       "                      -8.5386e-02, -1.4443e-01, -3.3022e-02, -7.4231e-02, -1.2408e-01,\n",
       "                       1.2135e-01,  4.5849e-02,  1.1428e-01, -6.8180e-02, -2.2709e-02,\n",
       "                      -6.2143e-02, -1.3191e-02, -5.3547e-02, -5.4072e-02,  1.3339e-02,\n",
       "                      -9.4828e-02,  8.1563e-02,  2.9594e-02,  4.1252e-02,  9.9738e-02,\n",
       "                      -4.0282e-02, -2.6957e-02, -1.5707e-02,  1.3024e-01,  2.0143e-02,\n",
       "                       5.4607e-02, -1.7511e-02, -2.6044e-02, -4.2827e-02, -1.4940e-02,\n",
       "                      -2.2724e-02, -1.3463e-01,  4.9154e-02,  5.0854e-02, -3.5467e-02,\n",
       "                       7.8202e-02,  2.6303e-02, -6.0555e-02,  8.2806e-02, -1.0146e-01,\n",
       "                      -1.3730e-01, -4.1975e-02, -4.0851e-02,  6.7782e-02,  4.7003e-02,\n",
       "                      -1.0495e-01,  1.2161e-01,  2.4026e-02,  1.6825e-01, -9.0507e-02,\n",
       "                       6.1903e-02,  6.5950e-02,  1.1059e-01])),\n",
       "             ('gru.bias_hh_l0',\n",
       "              tensor([ 0.0063, -0.0550, -0.0106,  0.1058,  0.0096, -0.0106,  0.1296,  0.0104,\n",
       "                       0.0099, -0.0138,  0.0493, -0.0105,  0.0365, -0.0293,  0.0578,  0.0004,\n",
       "                       0.0401, -0.0373,  0.1099,  0.1257,  0.0754,  0.0362,  0.0395, -0.0089,\n",
       "                      -0.0155,  0.0134,  0.0844,  0.0580,  0.0314, -0.0558,  0.0563, -0.0465,\n",
       "                       0.0130,  0.0596,  0.0223,  0.0280, -0.0259, -0.0454, -0.0488,  0.0071,\n",
       "                       0.0115, -0.0313,  0.0311,  0.0478,  0.0532,  0.0191,  0.0620, -0.0350,\n",
       "                       0.0790, -0.0497, -0.0531,  0.0482,  0.0235,  0.0490,  0.0676,  0.0358,\n",
       "                       0.0474,  0.0142,  0.0619, -0.0109,  0.0114,  0.0165,  0.0334, -0.0269,\n",
       "                       0.0340, -0.0096,  0.1516,  0.0685,  0.0663,  0.0192,  0.0283, -0.0054,\n",
       "                       0.0397,  0.0857,  0.0919,  0.0381, -0.0067, -0.0045,  0.0040,  0.0533,\n",
       "                       0.0520,  0.0281,  0.0732,  0.0206,  0.0414,  0.0393,  0.0206,  0.0317,\n",
       "                      -0.0230, -0.0109, -0.0185, -0.0286,  0.0292, -0.0502,  0.0318,  0.0759,\n",
       "                      -0.0253, -0.0335,  0.0467, -0.0126,  0.0126, -0.0296,  0.0361,  0.0294,\n",
       "                       0.0179, -0.0451,  0.0037, -0.0252,  0.0350, -0.0286,  0.0659,  0.0519,\n",
       "                      -0.0071,  0.0365,  0.0695,  0.0162, -0.0147,  0.0245, -0.0333,  0.0884,\n",
       "                       0.0161,  0.0607,  0.0066, -0.0135,  0.0701,  0.0425, -0.0242, -0.0248,\n",
       "                       0.0705,  0.0074,  0.0535,  0.0707, -0.0273,  0.0302,  0.0303,  0.0587,\n",
       "                       0.0459,  0.1080,  0.0249,  0.0310,  0.0805,  0.0167,  0.0445,  0.0498,\n",
       "                       0.0647,  0.0604, -0.0326, -0.0350, -0.0477, -0.0335, -0.0442, -0.0252,\n",
       "                       0.0482,  0.0433,  0.0396,  0.0310,  0.0596,  0.0140,  0.0274,  0.0207,\n",
       "                       0.0089,  0.0605,  0.0424,  0.0261, -0.0460,  0.0125,  0.0089, -0.0271,\n",
       "                       0.0719,  0.0777,  0.0606, -0.0385, -0.0168,  0.0151, -0.0372,  0.0407,\n",
       "                       0.0085, -0.0278, -0.0105, -0.0187, -0.0380, -0.0496,  0.0034,  0.0220,\n",
       "                       0.0394, -0.0225, -0.0240, -0.0174,  0.0379,  0.0350,  0.0030, -0.0138,\n",
       "                      -0.0189, -0.0542,  0.0318,  0.0527,  0.0128,  0.0682, -0.0029,  0.0082,\n",
       "                       0.0244,  0.0995,  0.0330,  0.0327,  0.0277,  0.1155, -0.0209,  0.0112,\n",
       "                      -0.0065,  0.0578,  0.0387, -0.0096, -0.0442,  0.0117, -0.0304,  0.0088,\n",
       "                       0.0505,  0.0309,  0.0265,  0.0375,  0.0694,  0.0251, -0.0337,  0.0480,\n",
       "                      -0.0083,  0.0209, -0.0333,  0.0753,  0.0702, -0.0395,  0.0041, -0.0345,\n",
       "                       0.0106,  0.1013, -0.0416, -0.0044, -0.0311, -0.0297,  0.0549,  0.0591,\n",
       "                       0.0612,  0.0003,  0.0529,  0.0946, -0.0322, -0.0295,  0.0437,  0.0560,\n",
       "                       0.0311,  0.0782, -0.0037,  0.0022,  0.0255,  0.0276,  0.0494, -0.0132,\n",
       "                      -0.0740, -0.0882,  0.0148, -0.1883, -0.0050, -0.0854,  0.1932, -0.0992,\n",
       "                      -0.0489, -0.0247, -0.0867, -0.1122, -0.0912, -0.0326, -0.1165, -0.0592,\n",
       "                      -0.0745, -0.0529, -0.1341, -0.1034, -0.0371, -0.0028, -0.0389, -0.0893,\n",
       "                      -0.0226, -0.0130, -0.1001, -0.0935, -0.0825, -0.1122, -0.1213, -0.0845,\n",
       "                      -0.0222, -0.0694, -0.0590, -0.0787, -0.0761, -0.0492, -0.0024,  0.0220,\n",
       "                       0.0082, -0.0203, -0.1155, -0.0378, -0.1197, -0.0995, -0.0092, -0.0996,\n",
       "                      -0.1191, -0.0839, -0.0852, -0.0918, -0.0119,  0.0004, -0.0393, -0.0263,\n",
       "                      -0.0412, -0.0329,  0.0350, -0.0489, -0.0216, -0.0189, -0.0337, -0.0131,\n",
       "                      -0.0131, -0.0355, -0.2462, -0.1010, -0.0457, -0.0359, -0.0106, -0.0534,\n",
       "                      -0.1036, -0.0596, -0.1211, -0.1019,  0.0087, -0.0707, -0.0615, -0.0330,\n",
       "                      -0.0999,  0.0365, -0.1251, -0.0809, -0.0096, -0.0718, -0.0552, -0.0866,\n",
       "                      -0.0937, -0.0487, -0.0991, -0.0474, -0.0908, -0.0777, -0.0120, -0.0542,\n",
       "                      -0.0751, -0.0580,  0.0159, -0.0113,  0.0056, -0.0645, -0.0264, -0.0282,\n",
       "                      -0.1024,  0.0160, -0.1414, -0.0303, -0.0684, -0.1189, -0.1510, -0.0675,\n",
       "                      -0.0131, -0.0712, -0.0451, -0.0416, -0.1386, -0.0059, -0.1032, -0.1185,\n",
       "                      -0.0552, -0.0752, -0.1452, -0.0069, -0.0405, -0.0655, -0.0697, -0.1029,\n",
       "                      -0.0108, -0.1392, -0.0755,  0.0116, -0.1335, -0.0495, -0.1259, -0.0266,\n",
       "                      -0.0427, -0.0471, -0.0355, -0.0994,  0.0024, -0.1106, -0.0600, -0.0800,\n",
       "                      -0.0857,  0.0164, -0.1012, -0.0792,  0.0215, -0.0769, -0.0014, -0.0245,\n",
       "                      -0.0262, -0.0530, -0.0465, -0.0941, -0.0364, -0.0493, -0.0598, -0.1523,\n",
       "                      -0.0798, -0.0482, -0.0558, -0.0865, -0.0467, -0.0718, -0.1096, -0.0856,\n",
       "                      -0.0402,  0.0185, -0.0026,  0.0083, -0.0652,  0.0298, -0.0482, -0.0357,\n",
       "                      -0.1689,  0.0203, -0.0507, -0.0785, -0.1059, -0.0027, -0.0935, -0.0563,\n",
       "                      -0.0691, -0.0811, -0.0497, -0.0061, -0.0189, -0.0528, -0.0542, -0.0699,\n",
       "                      -0.0175, -0.0426, -0.0761, -0.0339, -0.1358,  0.0044, -0.0950, -0.0080,\n",
       "                      -0.0498, -0.0433, -0.0212, -0.0693,  0.0049, -0.1879, -0.0052,  0.0339,\n",
       "                       0.0060, -0.0995, -0.0894, -0.1247, -0.0283, -0.0581, -0.0643, -0.0403,\n",
       "                      -0.0924, -0.0544, -0.0330, -0.0947, -0.0042, -0.1054, -0.0037, -0.0302,\n",
       "                      -0.0367, -0.0220, -0.0713,  0.0813, -0.0419, -0.0104,  0.0204, -0.1118,\n",
       "                      -0.0502, -0.0257, -0.0562, -0.0304,  0.0027, -0.0190,  0.0157, -0.0483,\n",
       "                      -0.0340, -0.0160, -0.0677, -0.1114, -0.1506, -0.0040, -0.0987, -0.0764,\n",
       "                      -0.0736, -0.0318, -0.1326, -0.0498, -0.0282, -0.0101, -0.1989, -0.0482,\n",
       "                       0.0012, -0.0004,  0.1749,  0.0004, -0.0409, -0.0973, -0.1492, -0.0292,\n",
       "                      -0.0137, -0.0110,  0.0684, -0.0347,  0.0240, -0.0253, -0.0909,  0.0054,\n",
       "                      -0.0337,  0.0200, -0.1412,  0.0953, -0.0332,  0.0278,  0.0319, -0.1116,\n",
       "                       0.0086, -0.0207,  0.0344, -0.1325,  0.0069,  0.0128,  0.0646,  0.0686,\n",
       "                       0.0762,  0.0843, -0.1180, -0.0319, -0.0530, -0.0880, -0.0130,  0.0036,\n",
       "                       0.0618, -0.0837, -0.1131, -0.0445, -0.0960, -0.0346,  0.0128,  0.0387,\n",
       "                       0.0829, -0.0208,  0.0599, -0.0465,  0.0700, -0.0671, -0.0985, -0.0578,\n",
       "                       0.0052,  0.0100, -0.0560,  0.0086, -0.0115, -0.0404,  0.0862,  0.0875,\n",
       "                       0.0430,  0.0149,  0.0309,  0.0735,  0.0293, -0.0193, -0.0290, -0.0194,\n",
       "                       0.0413,  0.0276,  0.0855,  0.0075, -0.1267, -0.0079,  0.0356,  0.0007,\n",
       "                      -0.0242,  0.0694, -0.1107, -0.0503,  0.0321, -0.0165,  0.0535, -0.0224,\n",
       "                      -0.1029,  0.0192, -0.1010,  0.0236,  0.0713, -0.0288,  0.0903,  0.0196,\n",
       "                       0.0017,  0.0349,  0.0620,  0.0283, -0.0418,  0.0204,  0.0372, -0.0124,\n",
       "                       0.0452,  0.0132, -0.0772,  0.0817, -0.1070,  0.0622,  0.0338, -0.0671,\n",
       "                       0.0685,  0.1015,  0.0302, -0.0078, -0.0816, -0.0543, -0.0243,  0.0009,\n",
       "                       0.0352, -0.0150,  0.0932,  0.0444, -0.0031,  0.0399, -0.0287, -0.0255,\n",
       "                      -0.0364,  0.0099,  0.0138, -0.0068, -0.0502, -0.0383,  0.0946, -0.0277,\n",
       "                      -0.0038, -0.0892, -0.0073, -0.0685,  0.0304,  0.1620, -0.0311,  0.0465,\n",
       "                      -0.0871,  0.0394,  0.0243,  0.0265, -0.0441,  0.0529,  0.0740, -0.0636,\n",
       "                       0.0709,  0.0276, -0.1199, -0.0190,  0.0362,  0.0262, -0.0245, -0.0614,\n",
       "                      -0.0547,  0.0765,  0.0686, -0.0416, -0.0090, -0.0410,  0.0171, -0.0257,\n",
       "                      -0.0928,  0.1098, -0.0638, -0.0219,  0.0260, -0.0267, -0.0480, -0.0258,\n",
       "                       0.0030, -0.0684, -0.0378,  0.0896,  0.0162,  0.0475,  0.0284,  0.0369,\n",
       "                      -0.0434,  0.0547, -0.1019,  0.0069, -0.0219, -0.0165, -0.0871, -0.0234,\n",
       "                       0.1218, -0.0148, -0.0663, -0.0320,  0.0655, -0.0821,  0.0168,  0.0143,\n",
       "                      -0.0530, -0.0228, -0.0783, -0.1057, -0.0427, -0.0660, -0.0207,  0.0022,\n",
       "                       0.1195, -0.0021,  0.0684, -0.0625,  0.0475,  0.0288,  0.0276, -0.0455,\n",
       "                      -0.0599,  0.0837, -0.0419, -0.0335, -0.0189, -0.0191,  0.1149, -0.0537,\n",
       "                       0.0351,  0.0183,  0.0194,  0.0061, -0.0315,  0.0384,  0.0089,  0.0280,\n",
       "                      -0.0135,  0.0561, -0.0012,  0.0474,  0.0648, -0.0041,  0.0854,  0.0468,\n",
       "                      -0.0859, -0.0008,  0.0043, -0.0211, -0.0063,  0.0444,  0.0466, -0.0510,\n",
       "                       0.0047,  0.0779,  0.0582, -0.0042, -0.0695,  0.0107,  0.0515,  0.0864]))])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b2064048-1164-4f33-adef-a0c39ffa8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[-0.1075, -1.4561, -0.9269,  ..., -0.5095,  2.1325, -0.2530],\n",
       "                      [ 1.0590, -2.3225,  2.6499,  ..., -1.7026, -0.4011, -1.4441],\n",
       "                      [-0.3590,  0.9642,  0.0719,  ..., -0.8602, -2.1549,  0.8839],\n",
       "                      ...,\n",
       "                      [-1.6855,  1.0948,  0.1890,  ...,  1.2942,  0.6984, -0.0205],\n",
       "                      [-1.1715, -0.4992,  1.0895,  ..., -0.9463,  0.3902, -0.2431],\n",
       "                      [ 1.1411, -0.2259, -1.5470,  ...,  1.7607, -0.5163, -0.9703]])),\n",
       "             ('gru.weight_ih_l0',\n",
       "              tensor([[-0.0435,  0.0142, -0.0385,  ..., -0.0007,  0.0494,  0.0118],\n",
       "                      [ 0.0231,  0.0476,  0.0293,  ...,  0.0334,  0.0247,  0.0224],\n",
       "                      [-0.0404, -0.0577,  0.0246,  ..., -0.0609,  0.0534,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.1048,  0.0294,  ...,  0.0401, -0.0350,  0.1113],\n",
       "                      [ 0.0081,  0.0061, -0.0638,  ..., -0.0464,  0.0555, -0.0692],\n",
       "                      [ 0.0490, -0.0380,  0.0497,  ...,  0.0124,  0.0720,  0.1041]])),\n",
       "             ('gru.weight_hh_l0',\n",
       "              tensor([[ 0.0171,  0.0121,  0.0017,  ..., -0.0471, -0.0235, -0.0246],\n",
       "                      [-0.0179,  0.0523,  0.0339,  ...,  0.0275,  0.0382, -0.0182],\n",
       "                      [-0.0224, -0.0390,  0.0652,  ..., -0.0200, -0.0697,  0.0094],\n",
       "                      ...,\n",
       "                      [-0.0223, -0.0837,  0.0795,  ...,  0.0311, -0.0277,  0.0496],\n",
       "                      [-0.0581,  0.0142,  0.0759,  ...,  0.0372, -0.0651,  0.0396],\n",
       "                      [ 0.0078, -0.0013,  0.0367,  ..., -0.0395, -0.0420,  0.0046]])),\n",
       "             ('gru.bias_ih_l0',\n",
       "              tensor([ 3.0973e-02, -4.8310e-02,  6.9925e-02,  8.3475e-02,  6.8531e-02,\n",
       "                       2.2193e-02,  1.4898e-01,  5.0024e-02, -3.0826e-02,  2.0506e-02,\n",
       "                       1.0355e-02,  1.1282e-02,  8.3116e-02,  4.0010e-02, -3.4442e-02,\n",
       "                       3.1285e-02,  3.1425e-02, -5.5288e-03, -5.5873e-03,  1.2434e-01,\n",
       "                       2.8309e-02,  2.1427e-04,  8.2363e-02, -3.7961e-02,  4.6129e-02,\n",
       "                       6.7838e-02, -2.0762e-02,  9.8445e-02, -9.9223e-03,  5.2815e-02,\n",
       "                       2.6661e-02, -1.4395e-03,  5.2649e-02,  4.2422e-02,  2.9512e-02,\n",
       "                      -2.7055e-02, -2.6372e-02,  9.3036e-03, -2.1571e-03,  1.8847e-03,\n",
       "                      -2.5345e-02, -6.7821e-03,  5.0284e-02, -1.2652e-02,  2.9727e-02,\n",
       "                       3.1625e-02,  5.2020e-02, -4.1657e-02,  3.2806e-02,  2.3711e-02,\n",
       "                       4.4365e-02,  5.0680e-02,  5.0452e-02, -1.3460e-02, -3.8177e-02,\n",
       "                       5.2154e-02,  3.7765e-02, -5.0237e-02,  8.1771e-02, -4.1957e-02,\n",
       "                      -4.7043e-02,  5.2669e-02, -1.4969e-02,  9.1032e-02,  6.0428e-02,\n",
       "                       3.0585e-02,  1.2259e-01,  6.8840e-02, -3.3964e-02, -2.1146e-02,\n",
       "                       5.4892e-02,  3.5278e-02, -2.6426e-02,  1.4018e-02,  4.7429e-02,\n",
       "                       2.3443e-02,  6.2397e-02, -3.7459e-02,  3.6675e-03,  2.6768e-02,\n",
       "                      -3.6832e-02,  2.2561e-02,  2.9001e-02,  5.8717e-02,  5.4169e-02,\n",
       "                      -2.3884e-02,  5.2052e-02, -2.6198e-03, -7.6988e-03, -3.9603e-02,\n",
       "                       1.5400e-02,  3.4957e-02,  9.7899e-02, -3.4316e-02, -1.0079e-02,\n",
       "                       6.7788e-02,  6.2233e-02,  5.9221e-02, -5.0395e-02, -4.2250e-02,\n",
       "                       4.6767e-02,  2.4103e-02, -8.7490e-03,  2.1632e-02,  7.9259e-02,\n",
       "                       3.8676e-02,  2.8144e-02,  4.3903e-02, -1.3396e-02, -1.0908e-02,\n",
       "                       4.0307e-02,  5.4926e-02,  4.6653e-02, -3.0030e-03,  1.6358e-03,\n",
       "                       5.9264e-02,  4.6087e-02,  4.7669e-02, -3.1855e-02,  3.2350e-03,\n",
       "                      -1.5149e-02,  1.7265e-02,  8.0106e-02,  8.6505e-02,  6.9860e-02,\n",
       "                      -3.6001e-03, -3.1422e-02,  1.8473e-02,  6.7385e-02,  3.7461e-02,\n",
       "                       5.5395e-02,  5.7843e-02,  7.6955e-02,  2.2788e-02,  4.4649e-02,\n",
       "                      -2.9816e-02, -2.5901e-02,  1.1752e-01, -3.9728e-04,  3.5645e-02,\n",
       "                       7.9939e-02,  2.4673e-02, -5.9999e-02,  4.2680e-02, -9.9992e-03,\n",
       "                       4.0236e-02,  6.6899e-02,  7.2811e-03, -4.0078e-03,  3.0496e-02,\n",
       "                      -9.1282e-03,  7.1630e-02,  6.3693e-02,  3.7094e-02,  1.1349e-01,\n",
       "                      -1.7691e-02, -3.3975e-02,  5.1992e-02, -2.4725e-02,  3.6947e-02,\n",
       "                      -8.2738e-03,  2.0678e-02, -3.4330e-02, -4.2139e-02,  2.2958e-02,\n",
       "                       1.9207e-02, -1.0386e-02, -3.7414e-02,  8.5089e-02, -4.5815e-03,\n",
       "                       4.2611e-02, -1.6832e-03,  1.0702e-02,  1.6211e-02, -2.3345e-04,\n",
       "                       3.3662e-02,  6.2863e-02, -3.5538e-02,  3.6953e-02,  5.6600e-02,\n",
       "                       3.1933e-02,  5.5476e-02, -2.9507e-02, -5.6525e-02,  1.2034e-02,\n",
       "                      -1.1156e-02, -2.1684e-02,  5.3204e-02,  5.4113e-02,  5.8928e-02,\n",
       "                       7.6794e-02, -4.1446e-02, -4.2342e-02,  5.3719e-02, -1.6490e-02,\n",
       "                      -2.2003e-02,  2.3000e-02,  6.2461e-02,  6.1886e-02, -3.6537e-02,\n",
       "                      -4.0628e-02,  1.0260e-01,  7.4059e-02,  1.8195e-02, -1.0334e-02,\n",
       "                       8.8897e-02, -9.0179e-03,  5.6320e-03,  9.0578e-03,  4.6681e-02,\n",
       "                       3.6690e-02, -1.8956e-02,  1.6376e-02,  5.1023e-02, -2.2552e-02,\n",
       "                      -3.2809e-02, -2.4664e-02,  3.1770e-02, -3.4288e-02,  1.0660e-02,\n",
       "                       6.3905e-02,  5.0060e-02,  1.8881e-02,  2.9570e-02,  5.7554e-02,\n",
       "                      -8.3631e-03, -6.1744e-03,  9.7494e-02,  1.2623e-02,  5.5275e-04,\n",
       "                       2.9502e-02,  3.6682e-02, -1.1881e-02,  8.9968e-02,  5.1912e-02,\n",
       "                       2.2340e-02,  5.8894e-02,  9.7203e-03,  6.2132e-02,  5.4779e-02,\n",
       "                       3.9853e-02, -2.3788e-02,  3.2569e-02,  8.8725e-02,  5.2161e-03,\n",
       "                      -2.5444e-02, -5.4180e-02,  7.1539e-02,  2.4261e-02, -2.7525e-02,\n",
       "                       1.3255e-02,  9.7390e-02,  3.1559e-02,  1.6374e-02,  6.6830e-02,\n",
       "                       4.7039e-02, -6.1270e-02, -9.3830e-02, -3.7698e-02, -2.3536e-01,\n",
       "                      -3.8554e-02, -1.1713e-01,  1.7577e-01, -1.1839e-01,  4.4562e-02,\n",
       "                      -7.7686e-03, -9.1764e-02, -1.0155e-01, -1.2389e-01,  5.9429e-03,\n",
       "                      -1.2181e-01, -1.3172e-01, -2.4267e-02, -1.1209e-02, -5.6128e-02,\n",
       "                      -1.3858e-01, -2.5702e-02, -1.6680e-03, -1.0435e-01, -1.0553e-01,\n",
       "                      -6.2173e-03, -3.3512e-02, -1.8718e-02, -3.8179e-03,  3.7230e-02,\n",
       "                      -8.1539e-02, -1.5832e-01, -2.5796e-02, -4.4829e-02, -9.3723e-02,\n",
       "                       3.5385e-02, -8.1218e-02,  4.0679e-03,  2.2178e-03, -1.0192e-01,\n",
       "                      -8.6588e-02, -9.0904e-02, -1.0401e-01, -1.1399e-01,  2.3196e-02,\n",
       "                      -1.9894e-02, -1.5619e-02, -5.4635e-02, -6.2189e-02, -2.9502e-02,\n",
       "                      -3.5750e-03, -1.4086e-01,  6.4772e-03, -5.5059e-03, -7.6179e-02,\n",
       "                      -8.1077e-02, -4.5923e-02, -1.9108e-02, -8.8737e-02, -1.7039e-02,\n",
       "                      -8.1062e-02, -8.3540e-02, -3.0798e-02, -1.2527e-01, -5.6161e-03,\n",
       "                      -1.2550e-01, -1.0998e-02, -1.8030e-01, -3.8618e-02, -4.3450e-02,\n",
       "                      -9.1300e-02, -5.1501e-02,  1.8569e-02, -6.9147e-02, -2.1448e-03,\n",
       "                      -9.1001e-02, -4.9085e-02, -6.7796e-02,  1.8831e-02, -1.0528e-01,\n",
       "                      -8.0865e-02, -1.1945e-02,  5.8342e-02, -1.2550e-01, -8.1618e-02,\n",
       "                      -6.5294e-02, -2.8351e-03, -1.0421e-01,  1.6912e-02, -4.9571e-02,\n",
       "                      -4.1358e-02, -7.6102e-02, -7.5397e-02, -1.2346e-01,  2.0210e-03,\n",
       "                      -2.7455e-02, -8.7476e-02, -7.5364e-02, -4.4412e-02, -3.1407e-02,\n",
       "                       3.1191e-02, -2.4310e-02, -4.2655e-02, -9.2870e-02,  4.3308e-03,\n",
       "                      -6.6845e-02, -5.8023e-02, -7.5334e-02, -2.5190e-02, -3.8103e-02,\n",
       "                      -5.8836e-02, -1.3448e-01, -7.5101e-03, -2.1507e-02, -8.9779e-02,\n",
       "                      -4.7189e-02, -1.3710e-01, -9.2455e-02,  2.5287e-03, -8.5002e-02,\n",
       "                      -1.2464e-01,  1.0077e-02, -8.2457e-03, -2.0569e-01, -5.3995e-02,\n",
       "                       1.2481e-02, -6.7420e-02, -7.6732e-02, -9.4608e-02, -2.4566e-02,\n",
       "                      -9.3275e-02, -7.4274e-02, -5.6131e-02, -5.6158e-02, -7.1771e-02,\n",
       "                      -7.3791e-03, -7.5715e-02, -2.7093e-02, -5.2455e-02,  6.2025e-03,\n",
       "                      -1.1397e-01, -7.9458e-02, -1.1618e-01, -7.9421e-02,  2.0462e-02,\n",
       "                      -8.3434e-02,  1.6223e-02, -4.5310e-02, -4.2014e-02,  1.8927e-03,\n",
       "                      -6.7138e-02, -1.2286e-01, -4.8179e-02, -2.1413e-02, -9.9877e-02,\n",
       "                      -7.8848e-02,  1.1653e-02, -4.6316e-02, -8.4587e-02, -6.8256e-02,\n",
       "                      -1.3256e-01, -5.5269e-02, -9.2310e-02, -9.8704e-02, -9.4953e-02,\n",
       "                      -7.4802e-02, -8.4000e-02, -9.9535e-02,  3.9711e-03, -6.6620e-02,\n",
       "                      -5.7456e-02, -3.6228e-02, -4.6339e-02,  5.9419e-03,  2.5709e-02,\n",
       "                       4.6245e-04,  2.3790e-03, -1.2164e-01,  5.6749e-02, -2.5829e-03,\n",
       "                      -1.0634e-02, -4.4489e-02, -2.4458e-02, -1.2312e-01,  1.4779e-02,\n",
       "                      -1.0042e-01,  1.3233e-02, -4.5185e-02, -3.4653e-03,  5.5758e-03,\n",
       "                      -5.1099e-02,  4.2460e-02, -2.9868e-02, -1.1615e-01, -6.2874e-02,\n",
       "                      -5.4523e-02, -7.8886e-02, -1.5897e-01,  3.1862e-03, -5.9269e-02,\n",
       "                      -2.0114e-02, -9.5583e-02, -7.9009e-02, -1.5459e-02, -8.1869e-02,\n",
       "                      -1.3593e-02, -1.0124e-01, -1.0910e-01,  2.7680e-02, -5.9500e-02,\n",
       "                       1.1095e-02,  2.5280e-03, -1.0579e-01, -8.9330e-03, -3.7396e-02,\n",
       "                      -5.4802e-02,  2.6132e-02, -9.7218e-03, -8.1800e-02,  5.5493e-02,\n",
       "                      -7.8332e-02,  3.3411e-02, -1.1560e-01, -8.2293e-02, -3.3887e-02,\n",
       "                      -5.4436e-02, -3.9134e-02, -1.0384e-01,  1.9306e-02, -4.0848e-02,\n",
       "                      -7.1774e-02, -5.3316e-02, -2.6519e-02, -8.8617e-02, -8.2551e-02,\n",
       "                      -1.2854e-01, -3.1554e-02, -6.2312e-02, -4.8592e-02, -5.2150e-02,\n",
       "                      -8.4253e-02,  4.4243e-02, -1.5415e-02, -4.1127e-02, -6.1024e-02,\n",
       "                      -1.0835e-01, -1.8361e-02, -5.4564e-02, -1.3099e-01, -1.2356e-01,\n",
       "                      -5.7569e-02, -3.4837e-02, -9.5550e-02, -8.0488e-02, -1.9145e-02,\n",
       "                      -1.1218e-01, -5.0645e-02,  7.4919e-02, -2.4268e-02,  2.8064e-01,\n",
       "                      -2.3048e-02, -1.4407e-01, -1.0909e-01, -3.1238e-01, -1.0985e-01,\n",
       "                      -1.8121e-02, -2.5806e-02,  2.3559e-02, -7.3675e-02, -1.1081e-01,\n",
       "                       3.7304e-02, -5.9285e-02, -6.9644e-03, -3.3516e-02, -6.1883e-02,\n",
       "                      -1.4977e-01,  1.5446e-01, -3.2190e-02,  1.7055e-01,  6.1997e-03,\n",
       "                      -9.1644e-02,  5.3933e-02, -2.5574e-02,  1.9956e-01, -1.3338e-01,\n",
       "                       1.1853e-01,  9.8209e-02,  1.1299e-01,  2.3894e-01,  1.3758e-01,\n",
       "                       2.5153e-02, -2.2786e-01,  2.4864e-02, -1.1268e-01, -1.5830e-01,\n",
       "                       4.8416e-02,  1.0090e-01, -2.5867e-04, -2.0452e-02, -1.1597e-01,\n",
       "                      -1.1335e-01, -1.1092e-01, -6.8242e-02,  1.6519e-01,  8.1336e-02,\n",
       "                       1.3009e-01,  1.2555e-02,  2.1345e-01, -1.0213e-01,  3.1776e-02,\n",
       "                      -4.9541e-02, -8.9672e-02, -3.7207e-02,  7.0153e-02, -4.6940e-02,\n",
       "                      -8.8087e-02,  3.7649e-02,  5.1981e-02, -5.6114e-02, -1.9099e-02,\n",
       "                       1.8535e-02,  4.8469e-02, -5.3935e-02,  1.9487e-02,  6.8070e-02,\n",
       "                       3.8362e-02,  5.3326e-02, -9.0704e-02,  1.1723e-02, -1.0153e-02,\n",
       "                      -1.2888e-02,  2.0924e-01, -6.4152e-02, -1.3094e-01, -6.9768e-02,\n",
       "                       5.9834e-02,  2.6752e-03,  1.3100e-02,  1.5930e-01, -1.6344e-01,\n",
       "                      -2.9345e-02,  3.7192e-02, -5.2641e-02,  5.3412e-02, -2.4532e-02,\n",
       "                      -1.0695e-01,  8.3791e-02, -1.7759e-01,  9.2454e-02,  1.6645e-02,\n",
       "                      -4.4382e-02,  6.9884e-02,  5.0523e-02, -4.6382e-02,  8.6608e-02,\n",
       "                       5.6979e-02,  4.0885e-02, -1.0039e-01,  1.3022e-02,  1.6698e-02,\n",
       "                       4.6792e-02, -6.4869e-02,  8.8206e-02,  1.1169e-02,  5.2261e-02,\n",
       "                      -2.1247e-01,  5.2677e-02,  9.4796e-02, -5.8519e-02, -5.3133e-02,\n",
       "                       9.0620e-02, -6.0390e-02,  8.0790e-03, -1.2445e-01,  1.1867e-02,\n",
       "                      -1.1712e-01,  6.0555e-03, -6.5715e-02,  3.7100e-02,  1.0535e-01,\n",
       "                       1.0600e-01, -9.7980e-02,  6.6321e-03, -9.1769e-02, -5.5014e-02,\n",
       "                      -8.7816e-02, -1.0135e-02,  6.3305e-03,  5.4908e-02, -5.6107e-02,\n",
       "                      -1.6338e-01,  6.4779e-02, -1.1851e-01,  7.9862e-02, -1.6619e-01,\n",
       "                       4.2845e-02, -1.4205e-02,  3.8753e-02,  2.7318e-01, -1.0470e-01,\n",
       "                      -1.9855e-02, -7.5490e-02,  7.3020e-02,  7.6088e-02,  5.4358e-02,\n",
       "                      -1.2050e-01,  1.0944e-01,  6.1827e-02, -1.1933e-01,  5.0185e-02,\n",
       "                      -7.3072e-02, -1.2359e-01, -4.4314e-02,  4.5311e-02,  5.3045e-02,\n",
       "                      -5.5919e-02,  2.2072e-02, -4.4253e-02,  3.9614e-02,  1.2768e-02,\n",
       "                      -6.0684e-02, -9.2866e-02, -1.3613e-02,  6.0721e-02,  3.3449e-02,\n",
       "                      -1.3632e-01,  1.1857e-01,  3.6806e-02, -1.2030e-01,  1.0041e-01,\n",
       "                      -2.0822e-01, -2.8895e-02, -5.1834e-02, -2.5027e-02, -6.6510e-02,\n",
       "                      -8.5927e-02,  1.9339e-01, -2.8578e-02,  8.3812e-02,  4.5714e-02,\n",
       "                      -1.7666e-02, -8.9172e-03,  6.7074e-02, -1.2611e-01,  3.3761e-02,\n",
       "                       9.7322e-02,  5.6637e-02, -4.1444e-02,  6.8806e-02,  6.6332e-02,\n",
       "                      -1.1724e-01, -1.8774e-02, -3.6366e-02,  1.1739e-01, -9.7627e-02,\n",
       "                       7.6434e-02,  8.9573e-02, -4.2590e-02, -7.2800e-02, -1.1720e-01,\n",
       "                      -8.5386e-02, -1.4443e-01, -3.3022e-02, -7.4231e-02, -1.2408e-01,\n",
       "                       1.2135e-01,  4.5849e-02,  1.1428e-01, -6.8180e-02, -2.2709e-02,\n",
       "                      -6.2143e-02, -1.3191e-02, -5.3547e-02, -5.4072e-02,  1.3339e-02,\n",
       "                      -9.4828e-02,  8.1563e-02,  2.9594e-02,  4.1252e-02,  9.9738e-02,\n",
       "                      -4.0282e-02, -2.6957e-02, -1.5707e-02,  1.3024e-01,  2.0143e-02,\n",
       "                       5.4607e-02, -1.7511e-02, -2.6044e-02, -4.2827e-02, -1.4940e-02,\n",
       "                      -2.2724e-02, -1.3463e-01,  4.9154e-02,  5.0854e-02, -3.5467e-02,\n",
       "                       7.8202e-02,  2.6303e-02, -6.0555e-02,  8.2806e-02, -1.0146e-01,\n",
       "                      -1.3730e-01, -4.1975e-02, -4.0851e-02,  6.7782e-02,  4.7003e-02,\n",
       "                      -1.0495e-01,  1.2161e-01,  2.4026e-02,  1.6825e-01, -9.0507e-02,\n",
       "                       6.1903e-02,  6.5950e-02,  1.1059e-01])),\n",
       "             ('gru.bias_hh_l0',\n",
       "              tensor([ 0.0063, -0.0550, -0.0106,  0.1058,  0.0096, -0.0106,  0.1296,  0.0104,\n",
       "                       0.0099, -0.0138,  0.0493, -0.0105,  0.0365, -0.0293,  0.0578,  0.0004,\n",
       "                       0.0401, -0.0373,  0.1099,  0.1257,  0.0754,  0.0362,  0.0395, -0.0089,\n",
       "                      -0.0155,  0.0134,  0.0844,  0.0580,  0.0314, -0.0558,  0.0563, -0.0465,\n",
       "                       0.0130,  0.0596,  0.0223,  0.0280, -0.0259, -0.0454, -0.0488,  0.0071,\n",
       "                       0.0115, -0.0313,  0.0311,  0.0478,  0.0532,  0.0191,  0.0620, -0.0350,\n",
       "                       0.0790, -0.0497, -0.0531,  0.0482,  0.0235,  0.0490,  0.0676,  0.0358,\n",
       "                       0.0474,  0.0142,  0.0619, -0.0109,  0.0114,  0.0165,  0.0334, -0.0269,\n",
       "                       0.0340, -0.0096,  0.1516,  0.0685,  0.0663,  0.0192,  0.0283, -0.0054,\n",
       "                       0.0397,  0.0857,  0.0919,  0.0381, -0.0067, -0.0045,  0.0040,  0.0533,\n",
       "                       0.0520,  0.0281,  0.0732,  0.0206,  0.0414,  0.0393,  0.0206,  0.0317,\n",
       "                      -0.0230, -0.0109, -0.0185, -0.0286,  0.0292, -0.0502,  0.0318,  0.0759,\n",
       "                      -0.0253, -0.0335,  0.0467, -0.0126,  0.0126, -0.0296,  0.0361,  0.0294,\n",
       "                       0.0179, -0.0451,  0.0037, -0.0252,  0.0350, -0.0286,  0.0659,  0.0519,\n",
       "                      -0.0071,  0.0365,  0.0695,  0.0162, -0.0147,  0.0245, -0.0333,  0.0884,\n",
       "                       0.0161,  0.0607,  0.0066, -0.0135,  0.0701,  0.0425, -0.0242, -0.0248,\n",
       "                       0.0705,  0.0074,  0.0535,  0.0707, -0.0273,  0.0302,  0.0303,  0.0587,\n",
       "                       0.0459,  0.1080,  0.0249,  0.0310,  0.0805,  0.0167,  0.0445,  0.0498,\n",
       "                       0.0647,  0.0604, -0.0326, -0.0350, -0.0477, -0.0335, -0.0442, -0.0252,\n",
       "                       0.0482,  0.0433,  0.0396,  0.0310,  0.0596,  0.0140,  0.0274,  0.0207,\n",
       "                       0.0089,  0.0605,  0.0424,  0.0261, -0.0460,  0.0125,  0.0089, -0.0271,\n",
       "                       0.0719,  0.0777,  0.0606, -0.0385, -0.0168,  0.0151, -0.0372,  0.0407,\n",
       "                       0.0085, -0.0278, -0.0105, -0.0187, -0.0380, -0.0496,  0.0034,  0.0220,\n",
       "                       0.0394, -0.0225, -0.0240, -0.0174,  0.0379,  0.0350,  0.0030, -0.0138,\n",
       "                      -0.0189, -0.0542,  0.0318,  0.0527,  0.0128,  0.0682, -0.0029,  0.0082,\n",
       "                       0.0244,  0.0995,  0.0330,  0.0327,  0.0277,  0.1155, -0.0209,  0.0112,\n",
       "                      -0.0065,  0.0578,  0.0387, -0.0096, -0.0442,  0.0117, -0.0304,  0.0088,\n",
       "                       0.0505,  0.0309,  0.0265,  0.0375,  0.0694,  0.0251, -0.0337,  0.0480,\n",
       "                      -0.0083,  0.0209, -0.0333,  0.0753,  0.0702, -0.0395,  0.0041, -0.0345,\n",
       "                       0.0106,  0.1013, -0.0416, -0.0044, -0.0311, -0.0297,  0.0549,  0.0591,\n",
       "                       0.0612,  0.0003,  0.0529,  0.0946, -0.0322, -0.0295,  0.0437,  0.0560,\n",
       "                       0.0311,  0.0782, -0.0037,  0.0022,  0.0255,  0.0276,  0.0494, -0.0132,\n",
       "                      -0.0740, -0.0882,  0.0148, -0.1883, -0.0050, -0.0854,  0.1932, -0.0992,\n",
       "                      -0.0489, -0.0247, -0.0867, -0.1122, -0.0912, -0.0326, -0.1165, -0.0592,\n",
       "                      -0.0745, -0.0529, -0.1341, -0.1034, -0.0371, -0.0028, -0.0389, -0.0893,\n",
       "                      -0.0226, -0.0130, -0.1001, -0.0935, -0.0825, -0.1122, -0.1213, -0.0845,\n",
       "                      -0.0222, -0.0694, -0.0590, -0.0787, -0.0761, -0.0492, -0.0024,  0.0220,\n",
       "                       0.0082, -0.0203, -0.1155, -0.0378, -0.1197, -0.0995, -0.0092, -0.0996,\n",
       "                      -0.1191, -0.0839, -0.0852, -0.0918, -0.0119,  0.0004, -0.0393, -0.0263,\n",
       "                      -0.0412, -0.0329,  0.0350, -0.0489, -0.0216, -0.0189, -0.0337, -0.0131,\n",
       "                      -0.0131, -0.0355, -0.2462, -0.1010, -0.0457, -0.0359, -0.0106, -0.0534,\n",
       "                      -0.1036, -0.0596, -0.1211, -0.1019,  0.0087, -0.0707, -0.0615, -0.0330,\n",
       "                      -0.0999,  0.0365, -0.1251, -0.0809, -0.0096, -0.0718, -0.0552, -0.0866,\n",
       "                      -0.0937, -0.0487, -0.0991, -0.0474, -0.0908, -0.0777, -0.0120, -0.0542,\n",
       "                      -0.0751, -0.0580,  0.0159, -0.0113,  0.0056, -0.0645, -0.0264, -0.0282,\n",
       "                      -0.1024,  0.0160, -0.1414, -0.0303, -0.0684, -0.1189, -0.1510, -0.0675,\n",
       "                      -0.0131, -0.0712, -0.0451, -0.0416, -0.1386, -0.0059, -0.1032, -0.1185,\n",
       "                      -0.0552, -0.0752, -0.1452, -0.0069, -0.0405, -0.0655, -0.0697, -0.1029,\n",
       "                      -0.0108, -0.1392, -0.0755,  0.0116, -0.1335, -0.0495, -0.1259, -0.0266,\n",
       "                      -0.0427, -0.0471, -0.0355, -0.0994,  0.0024, -0.1106, -0.0600, -0.0800,\n",
       "                      -0.0857,  0.0164, -0.1012, -0.0792,  0.0215, -0.0769, -0.0014, -0.0245,\n",
       "                      -0.0262, -0.0530, -0.0465, -0.0941, -0.0364, -0.0493, -0.0598, -0.1523,\n",
       "                      -0.0798, -0.0482, -0.0558, -0.0865, -0.0467, -0.0718, -0.1096, -0.0856,\n",
       "                      -0.0402,  0.0185, -0.0026,  0.0083, -0.0652,  0.0298, -0.0482, -0.0357,\n",
       "                      -0.1689,  0.0203, -0.0507, -0.0785, -0.1059, -0.0027, -0.0935, -0.0563,\n",
       "                      -0.0691, -0.0811, -0.0497, -0.0061, -0.0189, -0.0528, -0.0542, -0.0699,\n",
       "                      -0.0175, -0.0426, -0.0761, -0.0339, -0.1358,  0.0044, -0.0950, -0.0080,\n",
       "                      -0.0498, -0.0433, -0.0212, -0.0693,  0.0049, -0.1879, -0.0052,  0.0339,\n",
       "                       0.0060, -0.0995, -0.0894, -0.1247, -0.0283, -0.0581, -0.0643, -0.0403,\n",
       "                      -0.0924, -0.0544, -0.0330, -0.0947, -0.0042, -0.1054, -0.0037, -0.0302,\n",
       "                      -0.0367, -0.0220, -0.0713,  0.0813, -0.0419, -0.0104,  0.0204, -0.1118,\n",
       "                      -0.0502, -0.0257, -0.0562, -0.0304,  0.0027, -0.0190,  0.0157, -0.0483,\n",
       "                      -0.0340, -0.0160, -0.0677, -0.1114, -0.1506, -0.0040, -0.0987, -0.0764,\n",
       "                      -0.0736, -0.0318, -0.1326, -0.0498, -0.0282, -0.0101, -0.1989, -0.0482,\n",
       "                       0.0012, -0.0004,  0.1749,  0.0004, -0.0409, -0.0973, -0.1492, -0.0292,\n",
       "                      -0.0137, -0.0110,  0.0684, -0.0347,  0.0240, -0.0253, -0.0909,  0.0054,\n",
       "                      -0.0337,  0.0200, -0.1412,  0.0953, -0.0332,  0.0278,  0.0319, -0.1116,\n",
       "                       0.0086, -0.0207,  0.0344, -0.1325,  0.0069,  0.0128,  0.0646,  0.0686,\n",
       "                       0.0762,  0.0843, -0.1180, -0.0319, -0.0530, -0.0880, -0.0130,  0.0036,\n",
       "                       0.0618, -0.0837, -0.1131, -0.0445, -0.0960, -0.0346,  0.0128,  0.0387,\n",
       "                       0.0829, -0.0208,  0.0599, -0.0465,  0.0700, -0.0671, -0.0985, -0.0578,\n",
       "                       0.0052,  0.0100, -0.0560,  0.0086, -0.0115, -0.0404,  0.0862,  0.0875,\n",
       "                       0.0430,  0.0149,  0.0309,  0.0735,  0.0293, -0.0193, -0.0290, -0.0194,\n",
       "                       0.0413,  0.0276,  0.0855,  0.0075, -0.1267, -0.0079,  0.0356,  0.0007,\n",
       "                      -0.0242,  0.0694, -0.1107, -0.0503,  0.0321, -0.0165,  0.0535, -0.0224,\n",
       "                      -0.1029,  0.0192, -0.1010,  0.0236,  0.0713, -0.0288,  0.0903,  0.0196,\n",
       "                       0.0017,  0.0349,  0.0620,  0.0283, -0.0418,  0.0204,  0.0372, -0.0124,\n",
       "                       0.0452,  0.0132, -0.0772,  0.0817, -0.1070,  0.0622,  0.0338, -0.0671,\n",
       "                       0.0685,  0.1015,  0.0302, -0.0078, -0.0816, -0.0543, -0.0243,  0.0009,\n",
       "                       0.0352, -0.0150,  0.0932,  0.0444, -0.0031,  0.0399, -0.0287, -0.0255,\n",
       "                      -0.0364,  0.0099,  0.0138, -0.0068, -0.0502, -0.0383,  0.0946, -0.0277,\n",
       "                      -0.0038, -0.0892, -0.0073, -0.0685,  0.0304,  0.1620, -0.0311,  0.0465,\n",
       "                      -0.0871,  0.0394,  0.0243,  0.0265, -0.0441,  0.0529,  0.0740, -0.0636,\n",
       "                       0.0709,  0.0276, -0.1199, -0.0190,  0.0362,  0.0262, -0.0245, -0.0614,\n",
       "                      -0.0547,  0.0765,  0.0686, -0.0416, -0.0090, -0.0410,  0.0171, -0.0257,\n",
       "                      -0.0928,  0.1098, -0.0638, -0.0219,  0.0260, -0.0267, -0.0480, -0.0258,\n",
       "                       0.0030, -0.0684, -0.0378,  0.0896,  0.0162,  0.0475,  0.0284,  0.0369,\n",
       "                      -0.0434,  0.0547, -0.1019,  0.0069, -0.0219, -0.0165, -0.0871, -0.0234,\n",
       "                       0.1218, -0.0148, -0.0663, -0.0320,  0.0655, -0.0821,  0.0168,  0.0143,\n",
       "                      -0.0530, -0.0228, -0.0783, -0.1057, -0.0427, -0.0660, -0.0207,  0.0022,\n",
       "                       0.1195, -0.0021,  0.0684, -0.0625,  0.0475,  0.0288,  0.0276, -0.0455,\n",
       "                      -0.0599,  0.0837, -0.0419, -0.0335, -0.0189, -0.0191,  0.1149, -0.0537,\n",
       "                       0.0351,  0.0183,  0.0194,  0.0061, -0.0315,  0.0384,  0.0089,  0.0280,\n",
       "                      -0.0135,  0.0561, -0.0012,  0.0474,  0.0648, -0.0041,  0.0854,  0.0468,\n",
       "                      -0.0859, -0.0008,  0.0043, -0.0211, -0.0063,  0.0444,  0.0466, -0.0510,\n",
       "                       0.0047,  0.0779,  0.0582, -0.0042, -0.0695,  0.0107,  0.0515,  0.0864]))])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "encoder.load_state_dict(torch.load('AE_encoder.dict'))\n",
    "\n",
    "encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0c26193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [03:32, 4.07MB/s]                             \n",
      "100%|███████████████████████████████▉| 399999/400000 [00:07<00:00, 52504.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "vec = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5995cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embeddings = torch.nn.Embedding.from_pretrained(vec.vectors,freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60dab341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = my_embeddings\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fa9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2803\n",
      "fra 4345\n",
      "0m 10s (- 3m 19s) (1000 5%) 4.3112\n",
      "0m 21s (- 3m 13s) (2000 10%) 4.1056\n",
      "0m 32s (- 3m 4s) (3000 15%) 4.0276\n",
      "0m 43s (- 2m 54s) (4000 20%) 3.8823\n",
      "0m 55s (- 2m 45s) (5000 25%) 3.8376\n",
      "1m 6s (- 2m 34s) (6000 30%) 3.7878\n",
      "1m 17s (- 2m 23s) (7000 35%) 3.6717\n",
      "1m 28s (- 2m 13s) (8000 40%) 3.6597\n",
      "1m 40s (- 2m 3s) (9000 45%) 3.5932\n",
      "1m 52s (- 1m 52s) (10000 50%) 3.5514\n",
      "2m 4s (- 1m 41s) (11000 55%) 3.4503\n",
      "2m 17s (- 1m 31s) (12000 60%) 3.4005\n",
      "2m 29s (- 1m 20s) (13000 65%) 3.3915\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(encoder1, attn_decoder1, 20000, print_every=1000, plot_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
