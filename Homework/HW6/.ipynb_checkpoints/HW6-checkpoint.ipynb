{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8be85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3634a2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b96d22dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4439739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "770b774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52d3a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28f1ea2f-0796-437f-bac8-0be319b3caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, random_seed = 42):\n",
    "    train_split = 0.8\n",
    "    random_seed = 42\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    validation_split = .2\n",
    "    \n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(train_indices)\n",
    "    random.shuffle(val_indices)\n",
    "    \n",
    "    train = [dataset[i] for i in train_indices]\n",
    "    val = [dataset[j] for j in val_indices]\n",
    "    \n",
    "    return train, val\n",
    "\n",
    "#any(word in train for word in val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2dd05b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2803\n",
      "fra 4345\n"
     ]
    }
   ],
   "source": [
    "###ENGLISH TO ENGLISH WORKAROUND\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareAEData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[0])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    pairs = [[v,v] for v,k in pairs]\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab3e89af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she s my first love .', 'she s my first love .']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fe917-6002-4443-bef2-71f481715fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8870ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2ad5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de08ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "062bda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7527ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cbf81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5fe73920",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7f13ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, enc_learning_rate = 0.01, dec_learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "        \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=enc_learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=dec_learning_rate)\n",
    "    \n",
    "    train_, val_ = train_test_split(pairs, random_seed = 42)\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(train_))\n",
    "                      for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25d6ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77364c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "633cdf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97f3e9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 27s (- 8m 39s) (1000 5%) 4.2078\n",
      "0m 59s (- 8m 54s) (2000 10%) 3.6780\n",
      "1m 27s (- 8m 13s) (3000 15%) 3.4738\n",
      "1m 58s (- 7m 54s) (4000 20%) 3.2387\n",
      "2m 27s (- 7m 22s) (5000 25%) 3.1660\n",
      "2m 58s (- 6m 56s) (6000 30%) 3.0595\n",
      "3m 27s (- 6m 24s) (7000 35%) 2.9013\n",
      "3m 59s (- 5m 59s) (8000 40%) 2.8575\n",
      "4m 29s (- 5m 29s) (9000 45%) 2.7806\n",
      "5m 1s (- 5m 1s) (10000 50%) 2.7396\n",
      "5m 30s (- 4m 30s) (11000 55%) 2.5351\n",
      "6m 0s (- 4m 0s) (12000 60%) 2.4768\n",
      "6m 29s (- 3m 29s) (13000 65%) 2.4918\n",
      "6m 59s (- 2m 59s) (14000 70%) 2.3806\n",
      "7m 28s (- 2m 29s) (15000 75%) 2.3577\n",
      "8m 2s (- 2m 0s) (16000 80%) 2.3057\n",
      "8m 31s (- 1m 30s) (17000 85%) 2.1871\n",
      "9m 1s (- 1m 0s) (18000 90%) 2.1282\n",
      "9m 30s (- 0m 30s) (19000 95%) 2.0965\n",
      "10m 1s (- 0m 0s) (20000 100%) 2.1175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size =EncoderRNN\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(encoder1, attn_decoder1, 20000, print_every=1000, plot_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "99c86f8a-6c5b-472f-9cac-43e649f2e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2803\n",
      "fra 2803\n",
      "0m 26s (- 8m 24s) (1000 5%) 3.3736\n",
      "0m 53s (- 7m 59s) (2000 10%) 2.6057\n",
      "1m 22s (- 7m 47s) (3000 15%) 2.3832\n",
      "1m 49s (- 7m 17s) (4000 20%) 2.0870\n",
      "2m 17s (- 6m 51s) (5000 25%) 1.8544\n",
      "2m 41s (- 6m 17s) (6000 30%) 1.6948\n",
      "3m 11s (- 5m 55s) (7000 35%) 1.5535\n",
      "3m 36s (- 5m 24s) (8000 40%) 1.3963\n",
      "4m 2s (- 4m 56s) (9000 45%) 1.2833\n",
      "4m 28s (- 4m 28s) (10000 50%) 1.2095\n",
      "4m 52s (- 3m 59s) (11000 55%) 1.0793\n",
      "5m 22s (- 3m 35s) (12000 60%) 0.9578\n",
      "5m 48s (- 3m 7s) (13000 65%) 1.0049\n",
      "6m 21s (- 2m 43s) (14000 70%) 0.9358\n",
      "6m 46s (- 2m 15s) (15000 75%) 0.8592\n",
      "7m 17s (- 1m 49s) (16000 80%) 0.8767\n",
      "7m 43s (- 1m 21s) (17000 85%) 0.7649\n",
      "8m 13s (- 0m 54s) (18000 90%) 0.7003\n",
      "8m 38s (- 0m 27s) (19000 95%) 0.6576\n",
      "9m 7s (- 0m 0s) (20000 100%) 0.6555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "input_lang, output_lang, pairs = prepareAEData('eng', 'fra', False)\n",
    "\n",
    "AE_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "AE_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(AE_encoder, AE_decoder, 20000, print_every=1000, plot_every=1000)\n",
    "\n",
    "torch.save(AE_encoder.state_dict(), 'AE_encoder.dict')\n",
    "torch.save(AE_decoder.state_dict(), 'AE_decoder.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9254e525-a842-49e8-a15b-b7714f1a132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use AE_encoder and new decoder for translation\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "plot_losses = trainIters(AE_encoder, attn_decoder1, 100, print_every=1000, plot_every = 10, enc_learning_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe2745eb-4bf3-4b70-85fe-405ecd69dd29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on EncoderRNN in module __main__ object:\n",
      "\n",
      "class EncoderRNN(torch.nn.modules.module.Module)\n",
      " |  EncoderRNN(input_size, hidden_size)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EncoderRNN\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_size, hidden_size)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input, hidden)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  initHidden(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AE_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a16f876e-73b0-4810-bcb8-8c15c5eb6309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[-0.1075, -1.4561, -0.9269,  ..., -0.5095,  2.1325, -0.2530],\n",
       "                      [ 1.0590, -2.3225,  2.6499,  ..., -1.7026, -0.4011, -1.4441],\n",
       "                      [-0.3590,  0.9642,  0.0719,  ..., -0.8602, -2.1549,  0.8839],\n",
       "                      ...,\n",
       "                      [-1.6855,  1.0948,  0.1890,  ...,  1.2942,  0.6984, -0.0205],\n",
       "                      [-1.1715, -0.4992,  1.0895,  ..., -0.9463,  0.3902, -0.2431],\n",
       "                      [ 1.1411, -0.2259, -1.5470,  ...,  1.7607, -0.5163, -0.9703]])),\n",
       "             ('gru.weight_ih_l0',\n",
       "              tensor([[-0.0435,  0.0142, -0.0385,  ..., -0.0007,  0.0494,  0.0118],\n",
       "                      [ 0.0231,  0.0476,  0.0293,  ...,  0.0334,  0.0247,  0.0224],\n",
       "                      [-0.0404, -0.0577,  0.0246,  ..., -0.0609,  0.0534,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.1048,  0.0294,  ...,  0.0401, -0.0350,  0.1113],\n",
       "                      [ 0.0081,  0.0061, -0.0638,  ..., -0.0464,  0.0555, -0.0692],\n",
       "                      [ 0.0490, -0.0380,  0.0497,  ...,  0.0124,  0.0720,  0.1041]])),\n",
       "             ('gru.weight_hh_l0',\n",
       "              tensor([[ 0.0171,  0.0121,  0.0017,  ..., -0.0471, -0.0235, -0.0246],\n",
       "                      [-0.0179,  0.0523,  0.0339,  ...,  0.0275,  0.0382, -0.0182],\n",
       "                      [-0.0224, -0.0390,  0.0652,  ..., -0.0200, -0.0697,  0.0094],\n",
       "                      ...,\n",
       "                      [-0.0223, -0.0837,  0.0795,  ...,  0.0311, -0.0277,  0.0496],\n",
       "                      [-0.0581,  0.0142,  0.0759,  ...,  0.0372, -0.0651,  0.0396],\n",
       "                      [ 0.0078, -0.0013,  0.0367,  ..., -0.0395, -0.0420,  0.0046]])),\n",
       "             ('gru.bias_ih_l0',\n",
       "              tensor([ 3.0973e-02, -4.8310e-02,  6.9925e-02,  8.3475e-02,  6.8531e-02,\n",
       "                       2.2193e-02,  1.4898e-01,  5.0024e-02, -3.0826e-02,  2.0506e-02,\n",
       "                       1.0355e-02,  1.1282e-02,  8.3116e-02,  4.0010e-02, -3.4442e-02,\n",
       "                       3.1285e-02,  3.1425e-02, -5.5288e-03, -5.5873e-03,  1.2434e-01,\n",
       "                       2.8309e-02,  2.1427e-04,  8.2363e-02, -3.7961e-02,  4.6129e-02,\n",
       "                       6.7838e-02, -2.0762e-02,  9.8445e-02, -9.9223e-03,  5.2815e-02,\n",
       "                       2.6661e-02, -1.4395e-03,  5.2649e-02,  4.2422e-02,  2.9512e-02,\n",
       "                      -2.7055e-02, -2.6372e-02,  9.3036e-03, -2.1571e-03,  1.8847e-03,\n",
       "                      -2.5345e-02, -6.7821e-03,  5.0284e-02, -1.2652e-02,  2.9727e-02,\n",
       "                       3.1625e-02,  5.2020e-02, -4.1657e-02,  3.2806e-02,  2.3711e-02,\n",
       "                       4.4365e-02,  5.0680e-02,  5.0452e-02, -1.3460e-02, -3.8177e-02,\n",
       "                       5.2154e-02,  3.7765e-02, -5.0237e-02,  8.1771e-02, -4.1957e-02,\n",
       "                      -4.7043e-02,  5.2669e-02, -1.4969e-02,  9.1032e-02,  6.0428e-02,\n",
       "                       3.0585e-02,  1.2259e-01,  6.8840e-02, -3.3964e-02, -2.1146e-02,\n",
       "                       5.4892e-02,  3.5278e-02, -2.6426e-02,  1.4018e-02,  4.7429e-02,\n",
       "                       2.3443e-02,  6.2397e-02, -3.7459e-02,  3.6675e-03,  2.6768e-02,\n",
       "                      -3.6832e-02,  2.2561e-02,  2.9001e-02,  5.8717e-02,  5.4169e-02,\n",
       "                      -2.3884e-02,  5.2052e-02, -2.6198e-03, -7.6988e-03, -3.9603e-02,\n",
       "                       1.5400e-02,  3.4957e-02,  9.7899e-02, -3.4316e-02, -1.0079e-02,\n",
       "                       6.7788e-02,  6.2233e-02,  5.9221e-02, -5.0395e-02, -4.2250e-02,\n",
       "                       4.6767e-02,  2.4103e-02, -8.7490e-03,  2.1632e-02,  7.9259e-02,\n",
       "                       3.8676e-02,  2.8144e-02,  4.3903e-02, -1.3396e-02, -1.0908e-02,\n",
       "                       4.0307e-02,  5.4926e-02,  4.6653e-02, -3.0030e-03,  1.6358e-03,\n",
       "                       5.9264e-02,  4.6087e-02,  4.7669e-02, -3.1855e-02,  3.2350e-03,\n",
       "                      -1.5149e-02,  1.7265e-02,  8.0106e-02,  8.6505e-02,  6.9860e-02,\n",
       "                      -3.6001e-03, -3.1422e-02,  1.8473e-02,  6.7385e-02,  3.7461e-02,\n",
       "                       5.5395e-02,  5.7843e-02,  7.6955e-02,  2.2788e-02,  4.4649e-02,\n",
       "                      -2.9816e-02, -2.5901e-02,  1.1752e-01, -3.9728e-04,  3.5645e-02,\n",
       "                       7.9939e-02,  2.4673e-02, -5.9999e-02,  4.2680e-02, -9.9992e-03,\n",
       "                       4.0236e-02,  6.6899e-02,  7.2811e-03, -4.0078e-03,  3.0496e-02,\n",
       "                      -9.1282e-03,  7.1630e-02,  6.3693e-02,  3.7094e-02,  1.1349e-01,\n",
       "                      -1.7691e-02, -3.3975e-02,  5.1992e-02, -2.4725e-02,  3.6947e-02,\n",
       "                      -8.2738e-03,  2.0678e-02, -3.4330e-02, -4.2139e-02,  2.2958e-02,\n",
       "                       1.9207e-02, -1.0386e-02, -3.7414e-02,  8.5089e-02, -4.5815e-03,\n",
       "                       4.2611e-02, -1.6832e-03,  1.0702e-02,  1.6211e-02, -2.3345e-04,\n",
       "                       3.3662e-02,  6.2863e-02, -3.5538e-02,  3.6953e-02,  5.6600e-02,\n",
       "                       3.1933e-02,  5.5476e-02, -2.9507e-02, -5.6525e-02,  1.2034e-02,\n",
       "                      -1.1156e-02, -2.1684e-02,  5.3204e-02,  5.4113e-02,  5.8928e-02,\n",
       "                       7.6794e-02, -4.1446e-02, -4.2342e-02,  5.3719e-02, -1.6490e-02,\n",
       "                      -2.2003e-02,  2.3000e-02,  6.2461e-02,  6.1886e-02, -3.6537e-02,\n",
       "                      -4.0628e-02,  1.0260e-01,  7.4059e-02,  1.8195e-02, -1.0334e-02,\n",
       "                       8.8897e-02, -9.0179e-03,  5.6320e-03,  9.0578e-03,  4.6681e-02,\n",
       "                       3.6690e-02, -1.8956e-02,  1.6376e-02,  5.1023e-02, -2.2552e-02,\n",
       "                      -3.2809e-02, -2.4664e-02,  3.1770e-02, -3.4288e-02,  1.0660e-02,\n",
       "                       6.3905e-02,  5.0060e-02,  1.8881e-02,  2.9570e-02,  5.7554e-02,\n",
       "                      -8.3631e-03, -6.1744e-03,  9.7494e-02,  1.2623e-02,  5.5275e-04,\n",
       "                       2.9502e-02,  3.6682e-02, -1.1881e-02,  8.9968e-02,  5.1912e-02,\n",
       "                       2.2340e-02,  5.8894e-02,  9.7203e-03,  6.2132e-02,  5.4779e-02,\n",
       "                       3.9853e-02, -2.3788e-02,  3.2569e-02,  8.8725e-02,  5.2161e-03,\n",
       "                      -2.5444e-02, -5.4180e-02,  7.1539e-02,  2.4261e-02, -2.7525e-02,\n",
       "                       1.3255e-02,  9.7390e-02,  3.1559e-02,  1.6374e-02,  6.6830e-02,\n",
       "                       4.7039e-02, -6.1270e-02, -9.3830e-02, -3.7698e-02, -2.3536e-01,\n",
       "                      -3.8554e-02, -1.1713e-01,  1.7577e-01, -1.1839e-01,  4.4562e-02,\n",
       "                      -7.7686e-03, -9.1764e-02, -1.0155e-01, -1.2389e-01,  5.9429e-03,\n",
       "                      -1.2181e-01, -1.3172e-01, -2.4267e-02, -1.1209e-02, -5.6128e-02,\n",
       "                      -1.3858e-01, -2.5702e-02, -1.6680e-03, -1.0435e-01, -1.0553e-01,\n",
       "                      -6.2173e-03, -3.3512e-02, -1.8718e-02, -3.8179e-03,  3.7230e-02,\n",
       "                      -8.1539e-02, -1.5832e-01, -2.5796e-02, -4.4829e-02, -9.3723e-02,\n",
       "                       3.5385e-02, -8.1218e-02,  4.0679e-03,  2.2178e-03, -1.0192e-01,\n",
       "                      -8.6588e-02, -9.0904e-02, -1.0401e-01, -1.1399e-01,  2.3196e-02,\n",
       "                      -1.9894e-02, -1.5619e-02, -5.4635e-02, -6.2189e-02, -2.9502e-02,\n",
       "                      -3.5750e-03, -1.4086e-01,  6.4772e-03, -5.5059e-03, -7.6179e-02,\n",
       "                      -8.1077e-02, -4.5923e-02, -1.9108e-02, -8.8737e-02, -1.7039e-02,\n",
       "                      -8.1062e-02, -8.3540e-02, -3.0798e-02, -1.2527e-01, -5.6161e-03,\n",
       "                      -1.2550e-01, -1.0998e-02, -1.8030e-01, -3.8618e-02, -4.3450e-02,\n",
       "                      -9.1300e-02, -5.1501e-02,  1.8569e-02, -6.9147e-02, -2.1448e-03,\n",
       "                      -9.1001e-02, -4.9085e-02, -6.7796e-02,  1.8831e-02, -1.0528e-01,\n",
       "                      -8.0865e-02, -1.1945e-02,  5.8342e-02, -1.2550e-01, -8.1618e-02,\n",
       "                      -6.5294e-02, -2.8351e-03, -1.0421e-01,  1.6912e-02, -4.9571e-02,\n",
       "                      -4.1358e-02, -7.6102e-02, -7.5397e-02, -1.2346e-01,  2.0210e-03,\n",
       "                      -2.7455e-02, -8.7476e-02, -7.5364e-02, -4.4412e-02, -3.1407e-02,\n",
       "                       3.1191e-02, -2.4310e-02, -4.2655e-02, -9.2870e-02,  4.3308e-03,\n",
       "                      -6.6845e-02, -5.8023e-02, -7.5334e-02, -2.5190e-02, -3.8103e-02,\n",
       "                      -5.8836e-02, -1.3448e-01, -7.5101e-03, -2.1507e-02, -8.9779e-02,\n",
       "                      -4.7189e-02, -1.3710e-01, -9.2455e-02,  2.5287e-03, -8.5002e-02,\n",
       "                      -1.2464e-01,  1.0077e-02, -8.2457e-03, -2.0569e-01, -5.3995e-02,\n",
       "                       1.2481e-02, -6.7420e-02, -7.6732e-02, -9.4608e-02, -2.4566e-02,\n",
       "                      -9.3275e-02, -7.4274e-02, -5.6131e-02, -5.6158e-02, -7.1771e-02,\n",
       "                      -7.3791e-03, -7.5715e-02, -2.7093e-02, -5.2455e-02,  6.2025e-03,\n",
       "                      -1.1397e-01, -7.9458e-02, -1.1618e-01, -7.9421e-02,  2.0462e-02,\n",
       "                      -8.3434e-02,  1.6223e-02, -4.5310e-02, -4.2014e-02,  1.8927e-03,\n",
       "                      -6.7138e-02, -1.2286e-01, -4.8179e-02, -2.1413e-02, -9.9877e-02,\n",
       "                      -7.8848e-02,  1.1653e-02, -4.6316e-02, -8.4587e-02, -6.8256e-02,\n",
       "                      -1.3256e-01, -5.5269e-02, -9.2310e-02, -9.8704e-02, -9.4953e-02,\n",
       "                      -7.4802e-02, -8.4000e-02, -9.9535e-02,  3.9711e-03, -6.6620e-02,\n",
       "                      -5.7456e-02, -3.6228e-02, -4.6339e-02,  5.9419e-03,  2.5709e-02,\n",
       "                       4.6245e-04,  2.3790e-03, -1.2164e-01,  5.6749e-02, -2.5829e-03,\n",
       "                      -1.0634e-02, -4.4489e-02, -2.4458e-02, -1.2312e-01,  1.4779e-02,\n",
       "                      -1.0042e-01,  1.3233e-02, -4.5185e-02, -3.4653e-03,  5.5758e-03,\n",
       "                      -5.1099e-02,  4.2460e-02, -2.9868e-02, -1.1615e-01, -6.2874e-02,\n",
       "                      -5.4523e-02, -7.8886e-02, -1.5897e-01,  3.1862e-03, -5.9269e-02,\n",
       "                      -2.0114e-02, -9.5583e-02, -7.9009e-02, -1.5459e-02, -8.1869e-02,\n",
       "                      -1.3593e-02, -1.0124e-01, -1.0910e-01,  2.7680e-02, -5.9500e-02,\n",
       "                       1.1095e-02,  2.5280e-03, -1.0579e-01, -8.9330e-03, -3.7396e-02,\n",
       "                      -5.4802e-02,  2.6132e-02, -9.7218e-03, -8.1800e-02,  5.5493e-02,\n",
       "                      -7.8332e-02,  3.3411e-02, -1.1560e-01, -8.2293e-02, -3.3887e-02,\n",
       "                      -5.4436e-02, -3.9134e-02, -1.0384e-01,  1.9306e-02, -4.0848e-02,\n",
       "                      -7.1774e-02, -5.3316e-02, -2.6519e-02, -8.8617e-02, -8.2551e-02,\n",
       "                      -1.2854e-01, -3.1554e-02, -6.2312e-02, -4.8592e-02, -5.2150e-02,\n",
       "                      -8.4253e-02,  4.4243e-02, -1.5415e-02, -4.1127e-02, -6.1024e-02,\n",
       "                      -1.0835e-01, -1.8361e-02, -5.4564e-02, -1.3099e-01, -1.2356e-01,\n",
       "                      -5.7569e-02, -3.4837e-02, -9.5550e-02, -8.0488e-02, -1.9145e-02,\n",
       "                      -1.1218e-01, -5.0645e-02,  7.4919e-02, -2.4268e-02,  2.8064e-01,\n",
       "                      -2.3048e-02, -1.4407e-01, -1.0909e-01, -3.1238e-01, -1.0985e-01,\n",
       "                      -1.8121e-02, -2.5806e-02,  2.3559e-02, -7.3675e-02, -1.1081e-01,\n",
       "                       3.7304e-02, -5.9285e-02, -6.9644e-03, -3.3516e-02, -6.1883e-02,\n",
       "                      -1.4977e-01,  1.5446e-01, -3.2190e-02,  1.7055e-01,  6.1997e-03,\n",
       "                      -9.1644e-02,  5.3933e-02, -2.5574e-02,  1.9956e-01, -1.3338e-01,\n",
       "                       1.1853e-01,  9.8209e-02,  1.1299e-01,  2.3894e-01,  1.3758e-01,\n",
       "                       2.5153e-02, -2.2786e-01,  2.4864e-02, -1.1268e-01, -1.5830e-01,\n",
       "                       4.8416e-02,  1.0090e-01, -2.5867e-04, -2.0452e-02, -1.1597e-01,\n",
       "                      -1.1335e-01, -1.1092e-01, -6.8242e-02,  1.6519e-01,  8.1336e-02,\n",
       "                       1.3009e-01,  1.2555e-02,  2.1345e-01, -1.0213e-01,  3.1776e-02,\n",
       "                      -4.9541e-02, -8.9672e-02, -3.7207e-02,  7.0153e-02, -4.6940e-02,\n",
       "                      -8.8087e-02,  3.7649e-02,  5.1981e-02, -5.6114e-02, -1.9099e-02,\n",
       "                       1.8535e-02,  4.8469e-02, -5.3935e-02,  1.9487e-02,  6.8070e-02,\n",
       "                       3.8362e-02,  5.3326e-02, -9.0704e-02,  1.1723e-02, -1.0153e-02,\n",
       "                      -1.2888e-02,  2.0924e-01, -6.4152e-02, -1.3094e-01, -6.9768e-02,\n",
       "                       5.9834e-02,  2.6752e-03,  1.3100e-02,  1.5930e-01, -1.6344e-01,\n",
       "                      -2.9345e-02,  3.7192e-02, -5.2641e-02,  5.3412e-02, -2.4532e-02,\n",
       "                      -1.0695e-01,  8.3791e-02, -1.7759e-01,  9.2454e-02,  1.6645e-02,\n",
       "                      -4.4382e-02,  6.9884e-02,  5.0523e-02, -4.6382e-02,  8.6608e-02,\n",
       "                       5.6979e-02,  4.0885e-02, -1.0039e-01,  1.3022e-02,  1.6698e-02,\n",
       "                       4.6792e-02, -6.4869e-02,  8.8206e-02,  1.1169e-02,  5.2261e-02,\n",
       "                      -2.1247e-01,  5.2677e-02,  9.4796e-02, -5.8519e-02, -5.3133e-02,\n",
       "                       9.0620e-02, -6.0390e-02,  8.0790e-03, -1.2445e-01,  1.1867e-02,\n",
       "                      -1.1712e-01,  6.0555e-03, -6.5715e-02,  3.7100e-02,  1.0535e-01,\n",
       "                       1.0600e-01, -9.7980e-02,  6.6321e-03, -9.1769e-02, -5.5014e-02,\n",
       "                      -8.7816e-02, -1.0135e-02,  6.3305e-03,  5.4908e-02, -5.6107e-02,\n",
       "                      -1.6338e-01,  6.4779e-02, -1.1851e-01,  7.9862e-02, -1.6619e-01,\n",
       "                       4.2845e-02, -1.4205e-02,  3.8753e-02,  2.7318e-01, -1.0470e-01,\n",
       "                      -1.9855e-02, -7.5490e-02,  7.3020e-02,  7.6088e-02,  5.4358e-02,\n",
       "                      -1.2050e-01,  1.0944e-01,  6.1827e-02, -1.1933e-01,  5.0185e-02,\n",
       "                      -7.3072e-02, -1.2359e-01, -4.4314e-02,  4.5311e-02,  5.3045e-02,\n",
       "                      -5.5919e-02,  2.2072e-02, -4.4253e-02,  3.9614e-02,  1.2768e-02,\n",
       "                      -6.0684e-02, -9.2866e-02, -1.3613e-02,  6.0721e-02,  3.3449e-02,\n",
       "                      -1.3632e-01,  1.1857e-01,  3.6806e-02, -1.2030e-01,  1.0041e-01,\n",
       "                      -2.0822e-01, -2.8895e-02, -5.1834e-02, -2.5027e-02, -6.6510e-02,\n",
       "                      -8.5927e-02,  1.9339e-01, -2.8578e-02,  8.3812e-02,  4.5714e-02,\n",
       "                      -1.7666e-02, -8.9172e-03,  6.7074e-02, -1.2611e-01,  3.3761e-02,\n",
       "                       9.7322e-02,  5.6637e-02, -4.1444e-02,  6.8806e-02,  6.6332e-02,\n",
       "                      -1.1724e-01, -1.8774e-02, -3.6366e-02,  1.1739e-01, -9.7627e-02,\n",
       "                       7.6434e-02,  8.9573e-02, -4.2590e-02, -7.2800e-02, -1.1720e-01,\n",
       "                      -8.5386e-02, -1.4443e-01, -3.3022e-02, -7.4231e-02, -1.2408e-01,\n",
       "                       1.2135e-01,  4.5849e-02,  1.1428e-01, -6.8180e-02, -2.2709e-02,\n",
       "                      -6.2143e-02, -1.3191e-02, -5.3547e-02, -5.4072e-02,  1.3339e-02,\n",
       "                      -9.4828e-02,  8.1563e-02,  2.9594e-02,  4.1252e-02,  9.9738e-02,\n",
       "                      -4.0282e-02, -2.6957e-02, -1.5707e-02,  1.3024e-01,  2.0143e-02,\n",
       "                       5.4607e-02, -1.7511e-02, -2.6044e-02, -4.2827e-02, -1.4940e-02,\n",
       "                      -2.2724e-02, -1.3463e-01,  4.9154e-02,  5.0854e-02, -3.5467e-02,\n",
       "                       7.8202e-02,  2.6303e-02, -6.0555e-02,  8.2806e-02, -1.0146e-01,\n",
       "                      -1.3730e-01, -4.1975e-02, -4.0851e-02,  6.7782e-02,  4.7003e-02,\n",
       "                      -1.0495e-01,  1.2161e-01,  2.4026e-02,  1.6825e-01, -9.0507e-02,\n",
       "                       6.1903e-02,  6.5950e-02,  1.1059e-01])),\n",
       "             ('gru.bias_hh_l0',\n",
       "              tensor([ 0.0063, -0.0550, -0.0106,  0.1058,  0.0096, -0.0106,  0.1296,  0.0104,\n",
       "                       0.0099, -0.0138,  0.0493, -0.0105,  0.0365, -0.0293,  0.0578,  0.0004,\n",
       "                       0.0401, -0.0373,  0.1099,  0.1257,  0.0754,  0.0362,  0.0395, -0.0089,\n",
       "                      -0.0155,  0.0134,  0.0844,  0.0580,  0.0314, -0.0558,  0.0563, -0.0465,\n",
       "                       0.0130,  0.0596,  0.0223,  0.0280, -0.0259, -0.0454, -0.0488,  0.0071,\n",
       "                       0.0115, -0.0313,  0.0311,  0.0478,  0.0532,  0.0191,  0.0620, -0.0350,\n",
       "                       0.0790, -0.0497, -0.0531,  0.0482,  0.0235,  0.0490,  0.0676,  0.0358,\n",
       "                       0.0474,  0.0142,  0.0619, -0.0109,  0.0114,  0.0165,  0.0334, -0.0269,\n",
       "                       0.0340, -0.0096,  0.1516,  0.0685,  0.0663,  0.0192,  0.0283, -0.0054,\n",
       "                       0.0397,  0.0857,  0.0919,  0.0381, -0.0067, -0.0045,  0.0040,  0.0533,\n",
       "                       0.0520,  0.0281,  0.0732,  0.0206,  0.0414,  0.0393,  0.0206,  0.0317,\n",
       "                      -0.0230, -0.0109, -0.0185, -0.0286,  0.0292, -0.0502,  0.0318,  0.0759,\n",
       "                      -0.0253, -0.0335,  0.0467, -0.0126,  0.0126, -0.0296,  0.0361,  0.0294,\n",
       "                       0.0179, -0.0451,  0.0037, -0.0252,  0.0350, -0.0286,  0.0659,  0.0519,\n",
       "                      -0.0071,  0.0365,  0.0695,  0.0162, -0.0147,  0.0245, -0.0333,  0.0884,\n",
       "                       0.0161,  0.0607,  0.0066, -0.0135,  0.0701,  0.0425, -0.0242, -0.0248,\n",
       "                       0.0705,  0.0074,  0.0535,  0.0707, -0.0273,  0.0302,  0.0303,  0.0587,\n",
       "                       0.0459,  0.1080,  0.0249,  0.0310,  0.0805,  0.0167,  0.0445,  0.0498,\n",
       "                       0.0647,  0.0604, -0.0326, -0.0350, -0.0477, -0.0335, -0.0442, -0.0252,\n",
       "                       0.0482,  0.0433,  0.0396,  0.0310,  0.0596,  0.0140,  0.0274,  0.0207,\n",
       "                       0.0089,  0.0605,  0.0424,  0.0261, -0.0460,  0.0125,  0.0089, -0.0271,\n",
       "                       0.0719,  0.0777,  0.0606, -0.0385, -0.0168,  0.0151, -0.0372,  0.0407,\n",
       "                       0.0085, -0.0278, -0.0105, -0.0187, -0.0380, -0.0496,  0.0034,  0.0220,\n",
       "                       0.0394, -0.0225, -0.0240, -0.0174,  0.0379,  0.0350,  0.0030, -0.0138,\n",
       "                      -0.0189, -0.0542,  0.0318,  0.0527,  0.0128,  0.0682, -0.0029,  0.0082,\n",
       "                       0.0244,  0.0995,  0.0330,  0.0327,  0.0277,  0.1155, -0.0209,  0.0112,\n",
       "                      -0.0065,  0.0578,  0.0387, -0.0096, -0.0442,  0.0117, -0.0304,  0.0088,\n",
       "                       0.0505,  0.0309,  0.0265,  0.0375,  0.0694,  0.0251, -0.0337,  0.0480,\n",
       "                      -0.0083,  0.0209, -0.0333,  0.0753,  0.0702, -0.0395,  0.0041, -0.0345,\n",
       "                       0.0106,  0.1013, -0.0416, -0.0044, -0.0311, -0.0297,  0.0549,  0.0591,\n",
       "                       0.0612,  0.0003,  0.0529,  0.0946, -0.0322, -0.0295,  0.0437,  0.0560,\n",
       "                       0.0311,  0.0782, -0.0037,  0.0022,  0.0255,  0.0276,  0.0494, -0.0132,\n",
       "                      -0.0740, -0.0882,  0.0148, -0.1883, -0.0050, -0.0854,  0.1932, -0.0992,\n",
       "                      -0.0489, -0.0247, -0.0867, -0.1122, -0.0912, -0.0326, -0.1165, -0.0592,\n",
       "                      -0.0745, -0.0529, -0.1341, -0.1034, -0.0371, -0.0028, -0.0389, -0.0893,\n",
       "                      -0.0226, -0.0130, -0.1001, -0.0935, -0.0825, -0.1122, -0.1213, -0.0845,\n",
       "                      -0.0222, -0.0694, -0.0590, -0.0787, -0.0761, -0.0492, -0.0024,  0.0220,\n",
       "                       0.0082, -0.0203, -0.1155, -0.0378, -0.1197, -0.0995, -0.0092, -0.0996,\n",
       "                      -0.1191, -0.0839, -0.0852, -0.0918, -0.0119,  0.0004, -0.0393, -0.0263,\n",
       "                      -0.0412, -0.0329,  0.0350, -0.0489, -0.0216, -0.0189, -0.0337, -0.0131,\n",
       "                      -0.0131, -0.0355, -0.2462, -0.1010, -0.0457, -0.0359, -0.0106, -0.0534,\n",
       "                      -0.1036, -0.0596, -0.1211, -0.1019,  0.0087, -0.0707, -0.0615, -0.0330,\n",
       "                      -0.0999,  0.0365, -0.1251, -0.0809, -0.0096, -0.0718, -0.0552, -0.0866,\n",
       "                      -0.0937, -0.0487, -0.0991, -0.0474, -0.0908, -0.0777, -0.0120, -0.0542,\n",
       "                      -0.0751, -0.0580,  0.0159, -0.0113,  0.0056, -0.0645, -0.0264, -0.0282,\n",
       "                      -0.1024,  0.0160, -0.1414, -0.0303, -0.0684, -0.1189, -0.1510, -0.0675,\n",
       "                      -0.0131, -0.0712, -0.0451, -0.0416, -0.1386, -0.0059, -0.1032, -0.1185,\n",
       "                      -0.0552, -0.0752, -0.1452, -0.0069, -0.0405, -0.0655, -0.0697, -0.1029,\n",
       "                      -0.0108, -0.1392, -0.0755,  0.0116, -0.1335, -0.0495, -0.1259, -0.0266,\n",
       "                      -0.0427, -0.0471, -0.0355, -0.0994,  0.0024, -0.1106, -0.0600, -0.0800,\n",
       "                      -0.0857,  0.0164, -0.1012, -0.0792,  0.0215, -0.0769, -0.0014, -0.0245,\n",
       "                      -0.0262, -0.0530, -0.0465, -0.0941, -0.0364, -0.0493, -0.0598, -0.1523,\n",
       "                      -0.0798, -0.0482, -0.0558, -0.0865, -0.0467, -0.0718, -0.1096, -0.0856,\n",
       "                      -0.0402,  0.0185, -0.0026,  0.0083, -0.0652,  0.0298, -0.0482, -0.0357,\n",
       "                      -0.1689,  0.0203, -0.0507, -0.0785, -0.1059, -0.0027, -0.0935, -0.0563,\n",
       "                      -0.0691, -0.0811, -0.0497, -0.0061, -0.0189, -0.0528, -0.0542, -0.0699,\n",
       "                      -0.0175, -0.0426, -0.0761, -0.0339, -0.1358,  0.0044, -0.0950, -0.0080,\n",
       "                      -0.0498, -0.0433, -0.0212, -0.0693,  0.0049, -0.1879, -0.0052,  0.0339,\n",
       "                       0.0060, -0.0995, -0.0894, -0.1247, -0.0283, -0.0581, -0.0643, -0.0403,\n",
       "                      -0.0924, -0.0544, -0.0330, -0.0947, -0.0042, -0.1054, -0.0037, -0.0302,\n",
       "                      -0.0367, -0.0220, -0.0713,  0.0813, -0.0419, -0.0104,  0.0204, -0.1118,\n",
       "                      -0.0502, -0.0257, -0.0562, -0.0304,  0.0027, -0.0190,  0.0157, -0.0483,\n",
       "                      -0.0340, -0.0160, -0.0677, -0.1114, -0.1506, -0.0040, -0.0987, -0.0764,\n",
       "                      -0.0736, -0.0318, -0.1326, -0.0498, -0.0282, -0.0101, -0.1989, -0.0482,\n",
       "                       0.0012, -0.0004,  0.1749,  0.0004, -0.0409, -0.0973, -0.1492, -0.0292,\n",
       "                      -0.0137, -0.0110,  0.0684, -0.0347,  0.0240, -0.0253, -0.0909,  0.0054,\n",
       "                      -0.0337,  0.0200, -0.1412,  0.0953, -0.0332,  0.0278,  0.0319, -0.1116,\n",
       "                       0.0086, -0.0207,  0.0344, -0.1325,  0.0069,  0.0128,  0.0646,  0.0686,\n",
       "                       0.0762,  0.0843, -0.1180, -0.0319, -0.0530, -0.0880, -0.0130,  0.0036,\n",
       "                       0.0618, -0.0837, -0.1131, -0.0445, -0.0960, -0.0346,  0.0128,  0.0387,\n",
       "                       0.0829, -0.0208,  0.0599, -0.0465,  0.0700, -0.0671, -0.0985, -0.0578,\n",
       "                       0.0052,  0.0100, -0.0560,  0.0086, -0.0115, -0.0404,  0.0862,  0.0875,\n",
       "                       0.0430,  0.0149,  0.0309,  0.0735,  0.0293, -0.0193, -0.0290, -0.0194,\n",
       "                       0.0413,  0.0276,  0.0855,  0.0075, -0.1267, -0.0079,  0.0356,  0.0007,\n",
       "                      -0.0242,  0.0694, -0.1107, -0.0503,  0.0321, -0.0165,  0.0535, -0.0224,\n",
       "                      -0.1029,  0.0192, -0.1010,  0.0236,  0.0713, -0.0288,  0.0903,  0.0196,\n",
       "                       0.0017,  0.0349,  0.0620,  0.0283, -0.0418,  0.0204,  0.0372, -0.0124,\n",
       "                       0.0452,  0.0132, -0.0772,  0.0817, -0.1070,  0.0622,  0.0338, -0.0671,\n",
       "                       0.0685,  0.1015,  0.0302, -0.0078, -0.0816, -0.0543, -0.0243,  0.0009,\n",
       "                       0.0352, -0.0150,  0.0932,  0.0444, -0.0031,  0.0399, -0.0287, -0.0255,\n",
       "                      -0.0364,  0.0099,  0.0138, -0.0068, -0.0502, -0.0383,  0.0946, -0.0277,\n",
       "                      -0.0038, -0.0892, -0.0073, -0.0685,  0.0304,  0.1620, -0.0311,  0.0465,\n",
       "                      -0.0871,  0.0394,  0.0243,  0.0265, -0.0441,  0.0529,  0.0740, -0.0636,\n",
       "                       0.0709,  0.0276, -0.1199, -0.0190,  0.0362,  0.0262, -0.0245, -0.0614,\n",
       "                      -0.0547,  0.0765,  0.0686, -0.0416, -0.0090, -0.0410,  0.0171, -0.0257,\n",
       "                      -0.0928,  0.1098, -0.0638, -0.0219,  0.0260, -0.0267, -0.0480, -0.0258,\n",
       "                       0.0030, -0.0684, -0.0378,  0.0896,  0.0162,  0.0475,  0.0284,  0.0369,\n",
       "                      -0.0434,  0.0547, -0.1019,  0.0069, -0.0219, -0.0165, -0.0871, -0.0234,\n",
       "                       0.1218, -0.0148, -0.0663, -0.0320,  0.0655, -0.0821,  0.0168,  0.0143,\n",
       "                      -0.0530, -0.0228, -0.0783, -0.1057, -0.0427, -0.0660, -0.0207,  0.0022,\n",
       "                       0.1195, -0.0021,  0.0684, -0.0625,  0.0475,  0.0288,  0.0276, -0.0455,\n",
       "                      -0.0599,  0.0837, -0.0419, -0.0335, -0.0189, -0.0191,  0.1149, -0.0537,\n",
       "                       0.0351,  0.0183,  0.0194,  0.0061, -0.0315,  0.0384,  0.0089,  0.0280,\n",
       "                      -0.0135,  0.0561, -0.0012,  0.0474,  0.0648, -0.0041,  0.0854,  0.0468,\n",
       "                      -0.0859, -0.0008,  0.0043, -0.0211, -0.0063,  0.0444,  0.0466, -0.0510,\n",
       "                       0.0047,  0.0779,  0.0582, -0.0042, -0.0695,  0.0107,  0.0515,  0.0864]))])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b2064048-1164-4f33-adef-a0c39ffa8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[-0.1075, -1.4561, -0.9269,  ..., -0.5095,  2.1325, -0.2530],\n",
       "                      [ 1.0590, -2.3225,  2.6499,  ..., -1.7026, -0.4011, -1.4441],\n",
       "                      [-0.3590,  0.9642,  0.0719,  ..., -0.8602, -2.1549,  0.8839],\n",
       "                      ...,\n",
       "                      [-1.6855,  1.0948,  0.1890,  ...,  1.2942,  0.6984, -0.0205],\n",
       "                      [-1.1715, -0.4992,  1.0895,  ..., -0.9463,  0.3902, -0.2431],\n",
       "                      [ 1.1411, -0.2259, -1.5470,  ...,  1.7607, -0.5163, -0.9703]])),\n",
       "             ('gru.weight_ih_l0',\n",
       "              tensor([[-0.0435,  0.0142, -0.0385,  ..., -0.0007,  0.0494,  0.0118],\n",
       "                      [ 0.0231,  0.0476,  0.0293,  ...,  0.0334,  0.0247,  0.0224],\n",
       "                      [-0.0404, -0.0577,  0.0246,  ..., -0.0609,  0.0534,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.1048,  0.0294,  ...,  0.0401, -0.0350,  0.1113],\n",
       "                      [ 0.0081,  0.0061, -0.0638,  ..., -0.0464,  0.0555, -0.0692],\n",
       "                      [ 0.0490, -0.0380,  0.0497,  ...,  0.0124,  0.0720,  0.1041]])),\n",
       "             ('gru.weight_hh_l0',\n",
       "              tensor([[ 0.0171,  0.0121,  0.0017,  ..., -0.0471, -0.0235, -0.0246],\n",
       "                      [-0.0179,  0.0523,  0.0339,  ...,  0.0275,  0.0382, -0.0182],\n",
       "                      [-0.0224, -0.0390,  0.0652,  ..., -0.0200, -0.0697,  0.0094],\n",
       "                      ...,\n",
       "                      [-0.0223, -0.0837,  0.0795,  ...,  0.0311, -0.0277,  0.0496],\n",
       "                      [-0.0581,  0.0142,  0.0759,  ...,  0.0372, -0.0651,  0.0396],\n",
       "                      [ 0.0078, -0.0013,  0.0367,  ..., -0.0395, -0.0420,  0.0046]])),\n",
       "             ('gru.bias_ih_l0',\n",
       "              tensor([ 3.0973e-02, -4.8310e-02,  6.9925e-02,  8.3475e-02,  6.8531e-02,\n",
       "                       2.2193e-02,  1.4898e-01,  5.0024e-02, -3.0826e-02,  2.0506e-02,\n",
       "                       1.0355e-02,  1.1282e-02,  8.3116e-02,  4.0010e-02, -3.4442e-02,\n",
       "                       3.1285e-02,  3.1425e-02, -5.5288e-03, -5.5873e-03,  1.2434e-01,\n",
       "                       2.8309e-02,  2.1427e-04,  8.2363e-02, -3.7961e-02,  4.6129e-02,\n",
       "                       6.7838e-02, -2.0762e-02,  9.8445e-02, -9.9223e-03,  5.2815e-02,\n",
       "                       2.6661e-02, -1.4395e-03,  5.2649e-02,  4.2422e-02,  2.9512e-02,\n",
       "                      -2.7055e-02, -2.6372e-02,  9.3036e-03, -2.1571e-03,  1.8847e-03,\n",
       "                      -2.5345e-02, -6.7821e-03,  5.0284e-02, -1.2652e-02,  2.9727e-02,\n",
       "                       3.1625e-02,  5.2020e-02, -4.1657e-02,  3.2806e-02,  2.3711e-02,\n",
       "                       4.4365e-02,  5.0680e-02,  5.0452e-02, -1.3460e-02, -3.8177e-02,\n",
       "                       5.2154e-02,  3.7765e-02, -5.0237e-02,  8.1771e-02, -4.1957e-02,\n",
       "                      -4.7043e-02,  5.2669e-02, -1.4969e-02,  9.1032e-02,  6.0428e-02,\n",
       "                       3.0585e-02,  1.2259e-01,  6.8840e-02, -3.3964e-02, -2.1146e-02,\n",
       "                       5.4892e-02,  3.5278e-02, -2.6426e-02,  1.4018e-02,  4.7429e-02,\n",
       "                       2.3443e-02,  6.2397e-02, -3.7459e-02,  3.6675e-03,  2.6768e-02,\n",
       "                      -3.6832e-02,  2.2561e-02,  2.9001e-02,  5.8717e-02,  5.4169e-02,\n",
       "                      -2.3884e-02,  5.2052e-02, -2.6198e-03, -7.6988e-03, -3.9603e-02,\n",
       "                       1.5400e-02,  3.4957e-02,  9.7899e-02, -3.4316e-02, -1.0079e-02,\n",
       "                       6.7788e-02,  6.2233e-02,  5.9221e-02, -5.0395e-02, -4.2250e-02,\n",
       "                       4.6767e-02,  2.4103e-02, -8.7490e-03,  2.1632e-02,  7.9259e-02,\n",
       "                       3.8676e-02,  2.8144e-02,  4.3903e-02, -1.3396e-02, -1.0908e-02,\n",
       "                       4.0307e-02,  5.4926e-02,  4.6653e-02, -3.0030e-03,  1.6358e-03,\n",
       "                       5.9264e-02,  4.6087e-02,  4.7669e-02, -3.1855e-02,  3.2350e-03,\n",
       "                      -1.5149e-02,  1.7265e-02,  8.0106e-02,  8.6505e-02,  6.9860e-02,\n",
       "                      -3.6001e-03, -3.1422e-02,  1.8473e-02,  6.7385e-02,  3.7461e-02,\n",
       "                       5.5395e-02,  5.7843e-02,  7.6955e-02,  2.2788e-02,  4.4649e-02,\n",
       "                      -2.9816e-02, -2.5901e-02,  1.1752e-01, -3.9728e-04,  3.5645e-02,\n",
       "                       7.9939e-02,  2.4673e-02, -5.9999e-02,  4.2680e-02, -9.9992e-03,\n",
       "                       4.0236e-02,  6.6899e-02,  7.2811e-03, -4.0078e-03,  3.0496e-02,\n",
       "                      -9.1282e-03,  7.1630e-02,  6.3693e-02,  3.7094e-02,  1.1349e-01,\n",
       "                      -1.7691e-02, -3.3975e-02,  5.1992e-02, -2.4725e-02,  3.6947e-02,\n",
       "                      -8.2738e-03,  2.0678e-02, -3.4330e-02, -4.2139e-02,  2.2958e-02,\n",
       "                       1.9207e-02, -1.0386e-02, -3.7414e-02,  8.5089e-02, -4.5815e-03,\n",
       "                       4.2611e-02, -1.6832e-03,  1.0702e-02,  1.6211e-02, -2.3345e-04,\n",
       "                       3.3662e-02,  6.2863e-02, -3.5538e-02,  3.6953e-02,  5.6600e-02,\n",
       "                       3.1933e-02,  5.5476e-02, -2.9507e-02, -5.6525e-02,  1.2034e-02,\n",
       "                      -1.1156e-02, -2.1684e-02,  5.3204e-02,  5.4113e-02,  5.8928e-02,\n",
       "                       7.6794e-02, -4.1446e-02, -4.2342e-02,  5.3719e-02, -1.6490e-02,\n",
       "                      -2.2003e-02,  2.3000e-02,  6.2461e-02,  6.1886e-02, -3.6537e-02,\n",
       "                      -4.0628e-02,  1.0260e-01,  7.4059e-02,  1.8195e-02, -1.0334e-02,\n",
       "                       8.8897e-02, -9.0179e-03,  5.6320e-03,  9.0578e-03,  4.6681e-02,\n",
       "                       3.6690e-02, -1.8956e-02,  1.6376e-02,  5.1023e-02, -2.2552e-02,\n",
       "                      -3.2809e-02, -2.4664e-02,  3.1770e-02, -3.4288e-02,  1.0660e-02,\n",
       "                       6.3905e-02,  5.0060e-02,  1.8881e-02,  2.9570e-02,  5.7554e-02,\n",
       "                      -8.3631e-03, -6.1744e-03,  9.7494e-02,  1.2623e-02,  5.5275e-04,\n",
       "                       2.9502e-02,  3.6682e-02, -1.1881e-02,  8.9968e-02,  5.1912e-02,\n",
       "                       2.2340e-02,  5.8894e-02,  9.7203e-03,  6.2132e-02,  5.4779e-02,\n",
       "                       3.9853e-02, -2.3788e-02,  3.2569e-02,  8.8725e-02,  5.2161e-03,\n",
       "                      -2.5444e-02, -5.4180e-02,  7.1539e-02,  2.4261e-02, -2.7525e-02,\n",
       "                       1.3255e-02,  9.7390e-02,  3.1559e-02,  1.6374e-02,  6.6830e-02,\n",
       "                       4.7039e-02, -6.1270e-02, -9.3830e-02, -3.7698e-02, -2.3536e-01,\n",
       "                      -3.8554e-02, -1.1713e-01,  1.7577e-01, -1.1839e-01,  4.4562e-02,\n",
       "                      -7.7686e-03, -9.1764e-02, -1.0155e-01, -1.2389e-01,  5.9429e-03,\n",
       "                      -1.2181e-01, -1.3172e-01, -2.4267e-02, -1.1209e-02, -5.6128e-02,\n",
       "                      -1.3858e-01, -2.5702e-02, -1.6680e-03, -1.0435e-01, -1.0553e-01,\n",
       "                      -6.2173e-03, -3.3512e-02, -1.8718e-02, -3.8179e-03,  3.7230e-02,\n",
       "                      -8.1539e-02, -1.5832e-01, -2.5796e-02, -4.4829e-02, -9.3723e-02,\n",
       "                       3.5385e-02, -8.1218e-02,  4.0679e-03,  2.2178e-03, -1.0192e-01,\n",
       "                      -8.6588e-02, -9.0904e-02, -1.0401e-01, -1.1399e-01,  2.3196e-02,\n",
       "                      -1.9894e-02, -1.5619e-02, -5.4635e-02, -6.2189e-02, -2.9502e-02,\n",
       "                      -3.5750e-03, -1.4086e-01,  6.4772e-03, -5.5059e-03, -7.6179e-02,\n",
       "                      -8.1077e-02, -4.5923e-02, -1.9108e-02, -8.8737e-02, -1.7039e-02,\n",
       "                      -8.1062e-02, -8.3540e-02, -3.0798e-02, -1.2527e-01, -5.6161e-03,\n",
       "                      -1.2550e-01, -1.0998e-02, -1.8030e-01, -3.8618e-02, -4.3450e-02,\n",
       "                      -9.1300e-02, -5.1501e-02,  1.8569e-02, -6.9147e-02, -2.1448e-03,\n",
       "                      -9.1001e-02, -4.9085e-02, -6.7796e-02,  1.8831e-02, -1.0528e-01,\n",
       "                      -8.0865e-02, -1.1945e-02,  5.8342e-02, -1.2550e-01, -8.1618e-02,\n",
       "                      -6.5294e-02, -2.8351e-03, -1.0421e-01,  1.6912e-02, -4.9571e-02,\n",
       "                      -4.1358e-02, -7.6102e-02, -7.5397e-02, -1.2346e-01,  2.0210e-03,\n",
       "                      -2.7455e-02, -8.7476e-02, -7.5364e-02, -4.4412e-02, -3.1407e-02,\n",
       "                       3.1191e-02, -2.4310e-02, -4.2655e-02, -9.2870e-02,  4.3308e-03,\n",
       "                      -6.6845e-02, -5.8023e-02, -7.5334e-02, -2.5190e-02, -3.8103e-02,\n",
       "                      -5.8836e-02, -1.3448e-01, -7.5101e-03, -2.1507e-02, -8.9779e-02,\n",
       "                      -4.7189e-02, -1.3710e-01, -9.2455e-02,  2.5287e-03, -8.5002e-02,\n",
       "                      -1.2464e-01,  1.0077e-02, -8.2457e-03, -2.0569e-01, -5.3995e-02,\n",
       "                       1.2481e-02, -6.7420e-02, -7.6732e-02, -9.4608e-02, -2.4566e-02,\n",
       "                      -9.3275e-02, -7.4274e-02, -5.6131e-02, -5.6158e-02, -7.1771e-02,\n",
       "                      -7.3791e-03, -7.5715e-02, -2.7093e-02, -5.2455e-02,  6.2025e-03,\n",
       "                      -1.1397e-01, -7.9458e-02, -1.1618e-01, -7.9421e-02,  2.0462e-02,\n",
       "                      -8.3434e-02,  1.6223e-02, -4.5310e-02, -4.2014e-02,  1.8927e-03,\n",
       "                      -6.7138e-02, -1.2286e-01, -4.8179e-02, -2.1413e-02, -9.9877e-02,\n",
       "                      -7.8848e-02,  1.1653e-02, -4.6316e-02, -8.4587e-02, -6.8256e-02,\n",
       "                      -1.3256e-01, -5.5269e-02, -9.2310e-02, -9.8704e-02, -9.4953e-02,\n",
       "                      -7.4802e-02, -8.4000e-02, -9.9535e-02,  3.9711e-03, -6.6620e-02,\n",
       "                      -5.7456e-02, -3.6228e-02, -4.6339e-02,  5.9419e-03,  2.5709e-02,\n",
       "                       4.6245e-04,  2.3790e-03, -1.2164e-01,  5.6749e-02, -2.5829e-03,\n",
       "                      -1.0634e-02, -4.4489e-02, -2.4458e-02, -1.2312e-01,  1.4779e-02,\n",
       "                      -1.0042e-01,  1.3233e-02, -4.5185e-02, -3.4653e-03,  5.5758e-03,\n",
       "                      -5.1099e-02,  4.2460e-02, -2.9868e-02, -1.1615e-01, -6.2874e-02,\n",
       "                      -5.4523e-02, -7.8886e-02, -1.5897e-01,  3.1862e-03, -5.9269e-02,\n",
       "                      -2.0114e-02, -9.5583e-02, -7.9009e-02, -1.5459e-02, -8.1869e-02,\n",
       "                      -1.3593e-02, -1.0124e-01, -1.0910e-01,  2.7680e-02, -5.9500e-02,\n",
       "                       1.1095e-02,  2.5280e-03, -1.0579e-01, -8.9330e-03, -3.7396e-02,\n",
       "                      -5.4802e-02,  2.6132e-02, -9.7218e-03, -8.1800e-02,  5.5493e-02,\n",
       "                      -7.8332e-02,  3.3411e-02, -1.1560e-01, -8.2293e-02, -3.3887e-02,\n",
       "                      -5.4436e-02, -3.9134e-02, -1.0384e-01,  1.9306e-02, -4.0848e-02,\n",
       "                      -7.1774e-02, -5.3316e-02, -2.6519e-02, -8.8617e-02, -8.2551e-02,\n",
       "                      -1.2854e-01, -3.1554e-02, -6.2312e-02, -4.8592e-02, -5.2150e-02,\n",
       "                      -8.4253e-02,  4.4243e-02, -1.5415e-02, -4.1127e-02, -6.1024e-02,\n",
       "                      -1.0835e-01, -1.8361e-02, -5.4564e-02, -1.3099e-01, -1.2356e-01,\n",
       "                      -5.7569e-02, -3.4837e-02, -9.5550e-02, -8.0488e-02, -1.9145e-02,\n",
       "                      -1.1218e-01, -5.0645e-02,  7.4919e-02, -2.4268e-02,  2.8064e-01,\n",
       "                      -2.3048e-02, -1.4407e-01, -1.0909e-01, -3.1238e-01, -1.0985e-01,\n",
       "                      -1.8121e-02, -2.5806e-02,  2.3559e-02, -7.3675e-02, -1.1081e-01,\n",
       "                       3.7304e-02, -5.9285e-02, -6.9644e-03, -3.3516e-02, -6.1883e-02,\n",
       "                      -1.4977e-01,  1.5446e-01, -3.2190e-02,  1.7055e-01,  6.1997e-03,\n",
       "                      -9.1644e-02,  5.3933e-02, -2.5574e-02,  1.9956e-01, -1.3338e-01,\n",
       "                       1.1853e-01,  9.8209e-02,  1.1299e-01,  2.3894e-01,  1.3758e-01,\n",
       "                       2.5153e-02, -2.2786e-01,  2.4864e-02, -1.1268e-01, -1.5830e-01,\n",
       "                       4.8416e-02,  1.0090e-01, -2.5867e-04, -2.0452e-02, -1.1597e-01,\n",
       "                      -1.1335e-01, -1.1092e-01, -6.8242e-02,  1.6519e-01,  8.1336e-02,\n",
       "                       1.3009e-01,  1.2555e-02,  2.1345e-01, -1.0213e-01,  3.1776e-02,\n",
       "                      -4.9541e-02, -8.9672e-02, -3.7207e-02,  7.0153e-02, -4.6940e-02,\n",
       "                      -8.8087e-02,  3.7649e-02,  5.1981e-02, -5.6114e-02, -1.9099e-02,\n",
       "                       1.8535e-02,  4.8469e-02, -5.3935e-02,  1.9487e-02,  6.8070e-02,\n",
       "                       3.8362e-02,  5.3326e-02, -9.0704e-02,  1.1723e-02, -1.0153e-02,\n",
       "                      -1.2888e-02,  2.0924e-01, -6.4152e-02, -1.3094e-01, -6.9768e-02,\n",
       "                       5.9834e-02,  2.6752e-03,  1.3100e-02,  1.5930e-01, -1.6344e-01,\n",
       "                      -2.9345e-02,  3.7192e-02, -5.2641e-02,  5.3412e-02, -2.4532e-02,\n",
       "                      -1.0695e-01,  8.3791e-02, -1.7759e-01,  9.2454e-02,  1.6645e-02,\n",
       "                      -4.4382e-02,  6.9884e-02,  5.0523e-02, -4.6382e-02,  8.6608e-02,\n",
       "                       5.6979e-02,  4.0885e-02, -1.0039e-01,  1.3022e-02,  1.6698e-02,\n",
       "                       4.6792e-02, -6.4869e-02,  8.8206e-02,  1.1169e-02,  5.2261e-02,\n",
       "                      -2.1247e-01,  5.2677e-02,  9.4796e-02, -5.8519e-02, -5.3133e-02,\n",
       "                       9.0620e-02, -6.0390e-02,  8.0790e-03, -1.2445e-01,  1.1867e-02,\n",
       "                      -1.1712e-01,  6.0555e-03, -6.5715e-02,  3.7100e-02,  1.0535e-01,\n",
       "                       1.0600e-01, -9.7980e-02,  6.6321e-03, -9.1769e-02, -5.5014e-02,\n",
       "                      -8.7816e-02, -1.0135e-02,  6.3305e-03,  5.4908e-02, -5.6107e-02,\n",
       "                      -1.6338e-01,  6.4779e-02, -1.1851e-01,  7.9862e-02, -1.6619e-01,\n",
       "                       4.2845e-02, -1.4205e-02,  3.8753e-02,  2.7318e-01, -1.0470e-01,\n",
       "                      -1.9855e-02, -7.5490e-02,  7.3020e-02,  7.6088e-02,  5.4358e-02,\n",
       "                      -1.2050e-01,  1.0944e-01,  6.1827e-02, -1.1933e-01,  5.0185e-02,\n",
       "                      -7.3072e-02, -1.2359e-01, -4.4314e-02,  4.5311e-02,  5.3045e-02,\n",
       "                      -5.5919e-02,  2.2072e-02, -4.4253e-02,  3.9614e-02,  1.2768e-02,\n",
       "                      -6.0684e-02, -9.2866e-02, -1.3613e-02,  6.0721e-02,  3.3449e-02,\n",
       "                      -1.3632e-01,  1.1857e-01,  3.6806e-02, -1.2030e-01,  1.0041e-01,\n",
       "                      -2.0822e-01, -2.8895e-02, -5.1834e-02, -2.5027e-02, -6.6510e-02,\n",
       "                      -8.5927e-02,  1.9339e-01, -2.8578e-02,  8.3812e-02,  4.5714e-02,\n",
       "                      -1.7666e-02, -8.9172e-03,  6.7074e-02, -1.2611e-01,  3.3761e-02,\n",
       "                       9.7322e-02,  5.6637e-02, -4.1444e-02,  6.8806e-02,  6.6332e-02,\n",
       "                      -1.1724e-01, -1.8774e-02, -3.6366e-02,  1.1739e-01, -9.7627e-02,\n",
       "                       7.6434e-02,  8.9573e-02, -4.2590e-02, -7.2800e-02, -1.1720e-01,\n",
       "                      -8.5386e-02, -1.4443e-01, -3.3022e-02, -7.4231e-02, -1.2408e-01,\n",
       "                       1.2135e-01,  4.5849e-02,  1.1428e-01, -6.8180e-02, -2.2709e-02,\n",
       "                      -6.2143e-02, -1.3191e-02, -5.3547e-02, -5.4072e-02,  1.3339e-02,\n",
       "                      -9.4828e-02,  8.1563e-02,  2.9594e-02,  4.1252e-02,  9.9738e-02,\n",
       "                      -4.0282e-02, -2.6957e-02, -1.5707e-02,  1.3024e-01,  2.0143e-02,\n",
       "                       5.4607e-02, -1.7511e-02, -2.6044e-02, -4.2827e-02, -1.4940e-02,\n",
       "                      -2.2724e-02, -1.3463e-01,  4.9154e-02,  5.0854e-02, -3.5467e-02,\n",
       "                       7.8202e-02,  2.6303e-02, -6.0555e-02,  8.2806e-02, -1.0146e-01,\n",
       "                      -1.3730e-01, -4.1975e-02, -4.0851e-02,  6.7782e-02,  4.7003e-02,\n",
       "                      -1.0495e-01,  1.2161e-01,  2.4026e-02,  1.6825e-01, -9.0507e-02,\n",
       "                       6.1903e-02,  6.5950e-02,  1.1059e-01])),\n",
       "             ('gru.bias_hh_l0',\n",
       "              tensor([ 0.0063, -0.0550, -0.0106,  0.1058,  0.0096, -0.0106,  0.1296,  0.0104,\n",
       "                       0.0099, -0.0138,  0.0493, -0.0105,  0.0365, -0.0293,  0.0578,  0.0004,\n",
       "                       0.0401, -0.0373,  0.1099,  0.1257,  0.0754,  0.0362,  0.0395, -0.0089,\n",
       "                      -0.0155,  0.0134,  0.0844,  0.0580,  0.0314, -0.0558,  0.0563, -0.0465,\n",
       "                       0.0130,  0.0596,  0.0223,  0.0280, -0.0259, -0.0454, -0.0488,  0.0071,\n",
       "                       0.0115, -0.0313,  0.0311,  0.0478,  0.0532,  0.0191,  0.0620, -0.0350,\n",
       "                       0.0790, -0.0497, -0.0531,  0.0482,  0.0235,  0.0490,  0.0676,  0.0358,\n",
       "                       0.0474,  0.0142,  0.0619, -0.0109,  0.0114,  0.0165,  0.0334, -0.0269,\n",
       "                       0.0340, -0.0096,  0.1516,  0.0685,  0.0663,  0.0192,  0.0283, -0.0054,\n",
       "                       0.0397,  0.0857,  0.0919,  0.0381, -0.0067, -0.0045,  0.0040,  0.0533,\n",
       "                       0.0520,  0.0281,  0.0732,  0.0206,  0.0414,  0.0393,  0.0206,  0.0317,\n",
       "                      -0.0230, -0.0109, -0.0185, -0.0286,  0.0292, -0.0502,  0.0318,  0.0759,\n",
       "                      -0.0253, -0.0335,  0.0467, -0.0126,  0.0126, -0.0296,  0.0361,  0.0294,\n",
       "                       0.0179, -0.0451,  0.0037, -0.0252,  0.0350, -0.0286,  0.0659,  0.0519,\n",
       "                      -0.0071,  0.0365,  0.0695,  0.0162, -0.0147,  0.0245, -0.0333,  0.0884,\n",
       "                       0.0161,  0.0607,  0.0066, -0.0135,  0.0701,  0.0425, -0.0242, -0.0248,\n",
       "                       0.0705,  0.0074,  0.0535,  0.0707, -0.0273,  0.0302,  0.0303,  0.0587,\n",
       "                       0.0459,  0.1080,  0.0249,  0.0310,  0.0805,  0.0167,  0.0445,  0.0498,\n",
       "                       0.0647,  0.0604, -0.0326, -0.0350, -0.0477, -0.0335, -0.0442, -0.0252,\n",
       "                       0.0482,  0.0433,  0.0396,  0.0310,  0.0596,  0.0140,  0.0274,  0.0207,\n",
       "                       0.0089,  0.0605,  0.0424,  0.0261, -0.0460,  0.0125,  0.0089, -0.0271,\n",
       "                       0.0719,  0.0777,  0.0606, -0.0385, -0.0168,  0.0151, -0.0372,  0.0407,\n",
       "                       0.0085, -0.0278, -0.0105, -0.0187, -0.0380, -0.0496,  0.0034,  0.0220,\n",
       "                       0.0394, -0.0225, -0.0240, -0.0174,  0.0379,  0.0350,  0.0030, -0.0138,\n",
       "                      -0.0189, -0.0542,  0.0318,  0.0527,  0.0128,  0.0682, -0.0029,  0.0082,\n",
       "                       0.0244,  0.0995,  0.0330,  0.0327,  0.0277,  0.1155, -0.0209,  0.0112,\n",
       "                      -0.0065,  0.0578,  0.0387, -0.0096, -0.0442,  0.0117, -0.0304,  0.0088,\n",
       "                       0.0505,  0.0309,  0.0265,  0.0375,  0.0694,  0.0251, -0.0337,  0.0480,\n",
       "                      -0.0083,  0.0209, -0.0333,  0.0753,  0.0702, -0.0395,  0.0041, -0.0345,\n",
       "                       0.0106,  0.1013, -0.0416, -0.0044, -0.0311, -0.0297,  0.0549,  0.0591,\n",
       "                       0.0612,  0.0003,  0.0529,  0.0946, -0.0322, -0.0295,  0.0437,  0.0560,\n",
       "                       0.0311,  0.0782, -0.0037,  0.0022,  0.0255,  0.0276,  0.0494, -0.0132,\n",
       "                      -0.0740, -0.0882,  0.0148, -0.1883, -0.0050, -0.0854,  0.1932, -0.0992,\n",
       "                      -0.0489, -0.0247, -0.0867, -0.1122, -0.0912, -0.0326, -0.1165, -0.0592,\n",
       "                      -0.0745, -0.0529, -0.1341, -0.1034, -0.0371, -0.0028, -0.0389, -0.0893,\n",
       "                      -0.0226, -0.0130, -0.1001, -0.0935, -0.0825, -0.1122, -0.1213, -0.0845,\n",
       "                      -0.0222, -0.0694, -0.0590, -0.0787, -0.0761, -0.0492, -0.0024,  0.0220,\n",
       "                       0.0082, -0.0203, -0.1155, -0.0378, -0.1197, -0.0995, -0.0092, -0.0996,\n",
       "                      -0.1191, -0.0839, -0.0852, -0.0918, -0.0119,  0.0004, -0.0393, -0.0263,\n",
       "                      -0.0412, -0.0329,  0.0350, -0.0489, -0.0216, -0.0189, -0.0337, -0.0131,\n",
       "                      -0.0131, -0.0355, -0.2462, -0.1010, -0.0457, -0.0359, -0.0106, -0.0534,\n",
       "                      -0.1036, -0.0596, -0.1211, -0.1019,  0.0087, -0.0707, -0.0615, -0.0330,\n",
       "                      -0.0999,  0.0365, -0.1251, -0.0809, -0.0096, -0.0718, -0.0552, -0.0866,\n",
       "                      -0.0937, -0.0487, -0.0991, -0.0474, -0.0908, -0.0777, -0.0120, -0.0542,\n",
       "                      -0.0751, -0.0580,  0.0159, -0.0113,  0.0056, -0.0645, -0.0264, -0.0282,\n",
       "                      -0.1024,  0.0160, -0.1414, -0.0303, -0.0684, -0.1189, -0.1510, -0.0675,\n",
       "                      -0.0131, -0.0712, -0.0451, -0.0416, -0.1386, -0.0059, -0.1032, -0.1185,\n",
       "                      -0.0552, -0.0752, -0.1452, -0.0069, -0.0405, -0.0655, -0.0697, -0.1029,\n",
       "                      -0.0108, -0.1392, -0.0755,  0.0116, -0.1335, -0.0495, -0.1259, -0.0266,\n",
       "                      -0.0427, -0.0471, -0.0355, -0.0994,  0.0024, -0.1106, -0.0600, -0.0800,\n",
       "                      -0.0857,  0.0164, -0.1012, -0.0792,  0.0215, -0.0769, -0.0014, -0.0245,\n",
       "                      -0.0262, -0.0530, -0.0465, -0.0941, -0.0364, -0.0493, -0.0598, -0.1523,\n",
       "                      -0.0798, -0.0482, -0.0558, -0.0865, -0.0467, -0.0718, -0.1096, -0.0856,\n",
       "                      -0.0402,  0.0185, -0.0026,  0.0083, -0.0652,  0.0298, -0.0482, -0.0357,\n",
       "                      -0.1689,  0.0203, -0.0507, -0.0785, -0.1059, -0.0027, -0.0935, -0.0563,\n",
       "                      -0.0691, -0.0811, -0.0497, -0.0061, -0.0189, -0.0528, -0.0542, -0.0699,\n",
       "                      -0.0175, -0.0426, -0.0761, -0.0339, -0.1358,  0.0044, -0.0950, -0.0080,\n",
       "                      -0.0498, -0.0433, -0.0212, -0.0693,  0.0049, -0.1879, -0.0052,  0.0339,\n",
       "                       0.0060, -0.0995, -0.0894, -0.1247, -0.0283, -0.0581, -0.0643, -0.0403,\n",
       "                      -0.0924, -0.0544, -0.0330, -0.0947, -0.0042, -0.1054, -0.0037, -0.0302,\n",
       "                      -0.0367, -0.0220, -0.0713,  0.0813, -0.0419, -0.0104,  0.0204, -0.1118,\n",
       "                      -0.0502, -0.0257, -0.0562, -0.0304,  0.0027, -0.0190,  0.0157, -0.0483,\n",
       "                      -0.0340, -0.0160, -0.0677, -0.1114, -0.1506, -0.0040, -0.0987, -0.0764,\n",
       "                      -0.0736, -0.0318, -0.1326, -0.0498, -0.0282, -0.0101, -0.1989, -0.0482,\n",
       "                       0.0012, -0.0004,  0.1749,  0.0004, -0.0409, -0.0973, -0.1492, -0.0292,\n",
       "                      -0.0137, -0.0110,  0.0684, -0.0347,  0.0240, -0.0253, -0.0909,  0.0054,\n",
       "                      -0.0337,  0.0200, -0.1412,  0.0953, -0.0332,  0.0278,  0.0319, -0.1116,\n",
       "                       0.0086, -0.0207,  0.0344, -0.1325,  0.0069,  0.0128,  0.0646,  0.0686,\n",
       "                       0.0762,  0.0843, -0.1180, -0.0319, -0.0530, -0.0880, -0.0130,  0.0036,\n",
       "                       0.0618, -0.0837, -0.1131, -0.0445, -0.0960, -0.0346,  0.0128,  0.0387,\n",
       "                       0.0829, -0.0208,  0.0599, -0.0465,  0.0700, -0.0671, -0.0985, -0.0578,\n",
       "                       0.0052,  0.0100, -0.0560,  0.0086, -0.0115, -0.0404,  0.0862,  0.0875,\n",
       "                       0.0430,  0.0149,  0.0309,  0.0735,  0.0293, -0.0193, -0.0290, -0.0194,\n",
       "                       0.0413,  0.0276,  0.0855,  0.0075, -0.1267, -0.0079,  0.0356,  0.0007,\n",
       "                      -0.0242,  0.0694, -0.1107, -0.0503,  0.0321, -0.0165,  0.0535, -0.0224,\n",
       "                      -0.1029,  0.0192, -0.1010,  0.0236,  0.0713, -0.0288,  0.0903,  0.0196,\n",
       "                       0.0017,  0.0349,  0.0620,  0.0283, -0.0418,  0.0204,  0.0372, -0.0124,\n",
       "                       0.0452,  0.0132, -0.0772,  0.0817, -0.1070,  0.0622,  0.0338, -0.0671,\n",
       "                       0.0685,  0.1015,  0.0302, -0.0078, -0.0816, -0.0543, -0.0243,  0.0009,\n",
       "                       0.0352, -0.0150,  0.0932,  0.0444, -0.0031,  0.0399, -0.0287, -0.0255,\n",
       "                      -0.0364,  0.0099,  0.0138, -0.0068, -0.0502, -0.0383,  0.0946, -0.0277,\n",
       "                      -0.0038, -0.0892, -0.0073, -0.0685,  0.0304,  0.1620, -0.0311,  0.0465,\n",
       "                      -0.0871,  0.0394,  0.0243,  0.0265, -0.0441,  0.0529,  0.0740, -0.0636,\n",
       "                       0.0709,  0.0276, -0.1199, -0.0190,  0.0362,  0.0262, -0.0245, -0.0614,\n",
       "                      -0.0547,  0.0765,  0.0686, -0.0416, -0.0090, -0.0410,  0.0171, -0.0257,\n",
       "                      -0.0928,  0.1098, -0.0638, -0.0219,  0.0260, -0.0267, -0.0480, -0.0258,\n",
       "                       0.0030, -0.0684, -0.0378,  0.0896,  0.0162,  0.0475,  0.0284,  0.0369,\n",
       "                      -0.0434,  0.0547, -0.1019,  0.0069, -0.0219, -0.0165, -0.0871, -0.0234,\n",
       "                       0.1218, -0.0148, -0.0663, -0.0320,  0.0655, -0.0821,  0.0168,  0.0143,\n",
       "                      -0.0530, -0.0228, -0.0783, -0.1057, -0.0427, -0.0660, -0.0207,  0.0022,\n",
       "                       0.1195, -0.0021,  0.0684, -0.0625,  0.0475,  0.0288,  0.0276, -0.0455,\n",
       "                      -0.0599,  0.0837, -0.0419, -0.0335, -0.0189, -0.0191,  0.1149, -0.0537,\n",
       "                       0.0351,  0.0183,  0.0194,  0.0061, -0.0315,  0.0384,  0.0089,  0.0280,\n",
       "                      -0.0135,  0.0561, -0.0012,  0.0474,  0.0648, -0.0041,  0.0854,  0.0468,\n",
       "                      -0.0859, -0.0008,  0.0043, -0.0211, -0.0063,  0.0444,  0.0466, -0.0510,\n",
       "                       0.0047,  0.0779,  0.0582, -0.0042, -0.0695,  0.0107,  0.0515,  0.0864]))])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "encoder.load_state_dict(torch.load('AE_encoder.dict'))\n",
    "\n",
    "encoder.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
